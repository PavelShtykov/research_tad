{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tokens: 30000000, sample: 42857, iters (bs=4): 10714, steps (accum=8): 1339\n",
      "Approx hours to train: 3.0\n"
     ]
    }
   ],
   "source": [
    "toks = 30_000_000\n",
    "batch_to_second = 1\n",
    "avg_len_tokens = 700 \n",
    "micro_batch = 4 \n",
    "\n",
    "\n",
    "print(f'Target tokens: {toks}, sample: {toks / avg_len_tokens:.0f}, iters (bs=4): {toks / avg_len_tokens / 4:.0f}, steps (accum=8): {toks / avg_len_tokens / 4 / 8:.0f}')\n",
    "print(f\"Approx hours to train: {((toks / avg_len_tokens / micro_batch) / batch_to_second ) / 60 / 60:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "import litgpt\n",
    "# import litdata as ld\n",
    "# from litgpt.pretrain import initialize_weights\n",
    "import lightning as L\n",
    "import os\n",
    "from aux.arch_mod import Mods\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function aux.arch_mod.mod_first_layer_key_value(lit_model: aux.arch_mod.LitLLM)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Mods.first_layer_key_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual size of model: 30M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 512, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer_name = \"TinyLlama/TinyLlama_v1.1\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(tokenizer_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, max_len_single_sentence=2048)\n",
    "# tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.model_max_length = 2048\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab.__len__(),  # Use the same vocabulary size as the original model\n",
    "    hidden_size=512,\n",
    "    intermediate_size=1024*4,\n",
    "    num_hidden_layers=2,#12,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=8,\n",
    "    head_dim=64,\n",
    "    max_position_embeddings=tokenizer.model_max_length,  # Same as Llama-3\n",
    "    rms_norm_eps=1e-5,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    # Additional Llama-3 specific parameters\n",
    "    rope_theta=250000.0,\n",
    "    attention_bias=False,\n",
    "    tie_word_embeddings=True,\n",
    "    model_type=\"llama\",\n",
    "    _attn_implementation='sdpa'\n",
    "    # _attn_implementation_autoset=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([torch.prod(torch.tensor(p.size())) for p in model_parameters])\n",
    "    print(f\"Actual size of model: {params / 1024 ** 2:.0f}M\")\n",
    "\n",
    "count_trainable_parameters(model)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7b7ce0b03090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = None\n",
    "\n",
    "def hook(module, input, output):\n",
    "    global aaa\n",
    "    print(f'module: {module}, input: {input[0].shape}')\n",
    "    aaa = output\n",
    "     \n",
    "\n",
    "model.model.layers[0].self_attn.k_proj.register_forward_hook(hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(1, 1, 512)\n",
    "b = torch.zeros(1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([a[:, :-1, :], b[:, -1:, :]], -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1619,  1024,   338, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[32m      3\u001b[39m outputs = model(**inputs)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3830\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3808\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3811\u001b[39m     **kwargs,\n\u001b[32m   3812\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3815\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3828\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3830\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m   3831\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\n\u001b[32m   3838\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3831\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3808\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3811\u001b[39m     **kwargs,\n\u001b[32m   3812\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3815\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3828\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3830\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m3831\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   3838\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3870\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3867\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3868\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3870\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3875\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:668\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    667\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    671\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    672\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    674\u001b[39m )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mTypeError\u001b[39m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"My name is \", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "outputs = model(**inputs)\n",
    "print(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> My name is 当onio ApplicationFig shot Норaming DDRuß Christoph todoenden Vienengono Распо'}endorfneumango�\"]\n"
     ]
    }
   ],
   "source": [
    "model = mod_knn_as_lm_head(model)\n",
    "\n",
    "inputs = tokenizer(\"My name is \", return_tensors=\"pt\")\n",
    "outputs = provide_litllm_with_mod(config, total_steps=111, mods=['parallel_attention', 'knn_as_lm_head']).model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODS = ['adsf_sdfsdf_sfsf', 'sdfsdf_dfsfdbbg_Bbbbb']\n",
    "\n",
    "mods_str = '+'.join(\n",
    "    \"\".join([p[0] for p in m.split('_')]) \n",
    "    for m in MODS\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18*55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ass+sdB'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 512, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "import datasets\n",
    "# datasets.config.STREAMING_READ_MAX_RETRIES = 30\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "import os\n",
    "import logging\n",
    "from litgpt.utils import chunked_cross_entropy\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "GLOBAL_BATCH = 1024\n",
    "MICRO_BATCH = 12\n",
    "ACCUMULATED_BATCHES = GLOBAL_BATCH // MICRO_BATCH\n",
    "VAL_BATCH = 8\n",
    "TARGET_TRAIN_TOKENS = 300_000_000\n",
    "ITERS = TARGET_TRAIN_TOKENS // 700\n",
    "MAX_STEPS = ITERS // GLOBAL_BATCH\n",
    "MAX_LENGTH = 1024\n",
    "tokenizer_name = \"TinyLlama/TinyLlama_v1.1\"\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, max_len_single_sentence=MAX_LENGTH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "\n",
    "\n",
    "config = LlamaConfig(\n",
    "    vocab_size=tokenizer.vocab.__len__(),  # 32k size\n",
    "    hidden_size=512,\n",
    "    intermediate_size=512*4,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=16,\n",
    "    head_dim=32,\n",
    "    max_position_embeddings=tokenizer.model_max_length,  # 2048 tokens\n",
    "    rms_norm_eps=1e-5,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    # Additional Llama-3 specific parameters\n",
    "    rope_theta=250000.0,\n",
    "    attention_bias=False,\n",
    "    tie_word_embeddings=True,\n",
    "    model_type=\"llama\",\n",
    "    torch_dtype='float16',\n",
    "    _attn_implementation='eager'\n",
    "    # _attn_implementation_autoset=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "class LitLLM(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self._consumed_tokens = 0\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        predict = outputs['logits'][..., :-1, :]\n",
    "        target = batch[\"input_ids\"][..., 1:]\n",
    "        loss = chunked_cross_entropy(predict, target, chunk_size=256, ignore_index=self.model.config.pad_token_id)\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        # self.log('train/steps', batch_idx // ACCUMULATED_BATCHES, prog_bar=True)\n",
    "\n",
    "        self._consumed_tokens += batch['attention_mask'].sum().item()\n",
    "        self.log('train/consumed_tokens', self._consumed_tokens)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        outputs = self.model(**batch)\n",
    "        predict = outputs['logits'][..., :-1, :]\n",
    "        target = batch[\"input_ids\"][..., 1:]\n",
    "        loss = chunked_cross_entropy(predict, target, chunk_size=256, ignore_index=self.model.config.pad_token_id)\n",
    "\n",
    "        self.log(\"val/loss\", loss, prog_bar=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # warmup_steps = int(0.15 * MAX_STEPS)\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=4e-4, weight_decay=0.1, betas=(0.9, 0.95))\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, total_steps=MAX_STEPS, pct_start=0.1)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual size of model: 64M\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([torch.prod(torch.tensor(p.size())) for p in model_parameters])\n",
    "    print(f\"Actual size of model: {params / 1024 ** 2:.0f}M\")\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit_llm = LitLLM.load_from_checkpoint('base_microllama_150m/v_300Mtok_418steps_1024gbs/checkpoints/step=410-val_loss=5.63.ckpt', strict=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('base_microllama_64m/v_1000Mtok_1417steps_1008gbs/checkpoints/step=1403-val_loss=3.97.ckpt')\n",
    "print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict({k[6:]: v for k, v in checkpoint['state_dict'].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 512, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> London is 4-year, 24/7, starting next week in a large city, with a 8.7% to 5.4% drive from San Francisco for rent. The town has a 4.7% interest as a']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "inputs = tokenizer(\"London is \", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config=GenerationConfig(do_sample=True, max_new_tokens=50))\n",
    "print(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-04-17 18:15:53,022 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 29871, 29906,   718, 29871, 29906,   353,  1678, 29871, 29906,\n",
       "         29900, 29896, 29929, 29871, 29906, 29900, 29896, 29929, 29871, 29906,\n",
       "         29900, 29896, 29929, 29871, 29906, 29900, 29896, 29929]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlit_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/generation/utils.py:2329\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[32m   2324\u001b[39m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[32m   2325\u001b[39m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[32m   2326\u001b[39m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[32m   2327\u001b[39m max_cache_length = generation_config.max_length - \u001b[32m1\u001b[39m\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m2329\u001b[39m     \u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m != input_ids_length\n\u001b[32m   2330\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m model_input_name == \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2331\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder\n\u001b[32m   2332\u001b[39m ):\n\u001b[32m   2333\u001b[39m     max_cache_length += inputs_tensor.shape[\u001b[32m1\u001b[39m]\n\u001b[32m   2334\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_cache_for_generation(\n\u001b[32m   2335\u001b[39m     generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n\u001b[32m   2336\u001b[39m )\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "lit_llm.model.generate(torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('cerebras/SlimPajama-627B', split=f\"train\", streaming=True)\n",
    "train_dataset = (train_dataset\n",
    "    .shuffle(seed=42, buffer_size=50_000)\n",
    "    .map(\n",
    "        lambda x: tokenizer(x['text']), \n",
    "        batched=True, batch_size=1000\n",
    "    )\n",
    "    # .batch(MICRO_BATCH)\n",
    "    # .map(lambda batch: batch_padding(batch, tokenizer.pad_token_id, 2048))\n",
    ")\n",
    "val_dataset = load_dataset('cerebras/SlimPajama-627B', split=f\"validation\", streaming=True)\n",
    "val_dataset = (val_dataset\n",
    "    .shuffle(seed=42, buffer_size=10_000)\n",
    "    .map(\n",
    "        lambda x: tokenizer(x['text']), \n",
    "        batched=True, batch_size=100\n",
    "    )\n",
    "    # .batch(VAL_BATCH)\n",
    "    # .map(lambda batch: batch_padding(batch, tokenizer.pad_token_id, 2048))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428571"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1_000_000_000 // 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500000/1500000 [00:23<00:00, 62598.67it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "res = []\n",
    "skipped = 0\n",
    "it = iter(train_dataset)\n",
    "for _ in tqdm(range(1_500_000)):\n",
    "    try:    \n",
    "        res.append(next(it))\n",
    "    except:\n",
    "        skipped += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7301it [07:38, 20.89it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (136122 > 131072). Running this sequence through the model will result in indexing errors\n",
      "30400it [24:50, 20.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      3\u001b[39m res_val = []\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150_000\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:2266\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2263\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2266\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:1084\u001b[39m, in \u001b[36mMappedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter.format_row(pa_table)\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:1263\u001b[39m, in \u001b[36mMappedExamplesIterable._iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m   1258\u001b[39m     outputs = (\n\u001b[32m   1259\u001b[39m         (key, transformed_example)\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, transformed_batch \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[32m   1261\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m transformed_example \u001b[38;5;129;01min\u001b[39;00m _batch_to_examples(transformed_batch)\n\u001b[32m   1262\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_examples_since_previous_state\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:1258\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1256\u001b[39m outputs = iter_outputs()\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batched:\n\u001b[32m-> \u001b[39m\u001b[32m1258\u001b[39m     outputs = \u001b[43m(\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, transformed_example \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:1244\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.iter_outputs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1242\u001b[39m         \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mnum_examples_since_previous_state\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m   1243\u001b[39m         \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state_example_idx\u001b[39m\u001b[33m\"\u001b[39m] = current_idx\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:1113\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.iter_batched_inputs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;66;03m# If `batched`, first build the batch, if `batch_size` is None or <=0, then the batch is the whole dataset\u001b[39;00m\n\u001b[32m   1108\u001b[39m     iterator_batch = (\n\u001b[32m   1109\u001b[39m         iterator\n\u001b[32m   1110\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_size <= \u001b[32m0\u001b[39m\n\u001b[32m   1111\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m islice(iterator, \u001b[38;5;28mself\u001b[39m.batch_size - \u001b[32m1\u001b[39m)\n\u001b[32m   1112\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1113\u001b[39m     key_examples_list = [(key, example)] + \u001b[38;5;28mlist\u001b[39m(iterator_batch)\n\u001b[32m   1114\u001b[39m     keys, examples = \u001b[38;5;28mzip\u001b[39m(*key_examples_list)\n\u001b[32m   1115\u001b[39m     \u001b[38;5;66;03m# the new key is the concatenation of the examples keys from the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:1535\u001b[39m, in \u001b[36mBufferShuffledExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1533\u001b[39m \u001b[38;5;66;03m# this is the shuffle buffer that we keep in memory\u001b[39;00m\n\u001b[32m   1534\u001b[39m mem_buffer = []\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmem_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if the buffer is full, pick and example from it\u001b[39;49;00m\n\u001b[32m   1537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/iterable_dataset.py:374\u001b[39m, in \u001b[36mShuffledDataSourcesArrowExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    372\u001b[39m shard_example_idx_start = \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mshard_example_idx\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    373\u001b[39m shard_example_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tables_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/packaged_modules/json/json.py:123\u001b[39m, in \u001b[36mJson._generate_tables\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m    119\u001b[39m encoding_errors = (\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.encoding_errors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.encoding_errors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m )\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     batch = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n\u001b[32m    125\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/datasets/utils/file_utils.py:827\u001b[39m, in \u001b[36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_retries + \u001b[32m1\u001b[39m):\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         out = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    830\u001b[39m         aiohttp.client_exceptions.ClientError,\n\u001b[32m    831\u001b[39m         asyncio.TimeoutError,\n\u001b[32m    832\u001b[39m         requests.exceptions.ConnectionError,\n\u001b[32m    833\u001b[39m         requests.exceptions.Timeout,\n\u001b[32m    834\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:1013\u001b[39m, in \u001b[36mHfFileSystemFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   1011\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fs.open(\u001b[38;5;28mself\u001b[39m.path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m, block_size=\u001b[32m0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# block_size=0 enables fast streaming\u001b[39;00m\n\u001b[32m   1012\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/fsspec/spec.py:2083\u001b[39m, in \u001b[36mAbstractBufferedFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m length == \u001b[32m0\u001b[39m:\n\u001b[32m   2081\u001b[39m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[32m   2082\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2085\u001b[39m logger.debug(\n\u001b[32m   2086\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2087\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2090\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache._log_stats(),\n\u001b[32m   2091\u001b[39m )\n\u001b[32m   2092\u001b[39m \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/fsspec/caching.py:249\u001b[39m, in \u001b[36mReadAheadCache._fetch\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    247\u001b[39m end = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.size, end + \u001b[38;5;28mself\u001b[39m.blocksize)\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m.total_requested_bytes += end - start\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28mself\u001b[39m.cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28mself\u001b[39m.start = start\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.end = \u001b[38;5;28mself\u001b[39m.start + \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:969\u001b[39m, in \u001b[36mHfFileSystemFile._fetch_range\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    958\u001b[39m headers = {\n\u001b[32m    959\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrange\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    960\u001b[39m     **\u001b[38;5;28mself\u001b[39m.fs._api._build_hf_headers(),\n\u001b[32m    961\u001b[39m }\n\u001b[32m    962\u001b[39m url = hf_hub_url(\n\u001b[32m    963\u001b[39m     repo_id=\u001b[38;5;28mself\u001b[39m.resolved_path.repo_id,\n\u001b[32m    964\u001b[39m     revision=\u001b[38;5;28mself\u001b[39m.resolved_path.revision,\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m     endpoint=\u001b[38;5;28mself\u001b[39m.fs.endpoint,\n\u001b[32m    968\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m r = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m504\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    976\u001b[39m hf_raise_for_status(r)\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:310\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "res_val = []\n",
    "for i, batch in enumerate(tqdm(val_dataset)):\n",
    "    if i >= 150_000:\n",
    "        break\n",
    "\n",
    "    res_val.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import SplitInfo, DatasetInfo, NamedSplit\n",
    "import sys\n",
    "\n",
    "inmemory_300k_val = Dataset.from_list(\n",
    "    res_val, \n",
    "    info=DatasetInfo(\n",
    "        dataset_name='SlimPajama-627B',\n",
    "        description='first 30400 with shuffle(seed=42, buffer_size=10_000) and llama3 tokenizer',\n",
    "        dataset_size=len(res_val),\n",
    "        size_in_bytes=sys.getsizeof(res_val)\n",
    "        ),\n",
    "    split=NamedSplit('validation')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 27.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 28.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 28.13ba/s]\n"
     ]
    }
   ],
   "source": [
    "num_shards = 3\n",
    "output_path_template = \"slim_pajama_300k/validation/{index:05d}.parquet\"\n",
    "for index in range(num_shards):\n",
    "    shard = inmemory_300k_val.shard(index=index, num_shards=num_shards, contiguous=True)\n",
    "    shard.to_parquet(output_path_template.format(index=index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 300000/300000 [02:09<00:00, 2320.51 examples/s]\n",
      "Filter: 100%|██████████| 30400/30400 [00:13<00:00, 2298.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"train/*.parquet\", \"validation\": \"validation/*.parquet\"}\n",
    "test_ds: Dataset = load_dataset(\"slim_pajama_300k\", data_files=data_files, num_proc=8)\n",
    "test_ds = test_ds.filter(\n",
    " lambda b: b['meta']['redpajama_set_name'] not in {'RedPajamaStackExchange', 'RedPajamaGithub', 'RedPajamaArXiv'} \n",
    ")\n",
    "\n",
    "# 'RedPajamaArXiv',\n",
    "#  'RedPajamaBook',\n",
    "#  'RedPajamaC4',\n",
    "#  'RedPajamaCommonCrawl',\n",
    "#  'RedPajamaGithub',\n",
    "#  'RedPajamaStackExchange',\n",
    "#  'RedPajamaWikipedia'\n",
    "# test_ds = test_ds.remove_columns(['text', 'meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27651"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds['validation'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RedPajamaArXiv',\n",
       " 'RedPajamaBook',\n",
       " 'RedPajamaC4',\n",
       " 'RedPajamaCommonCrawl',\n",
       " 'RedPajamaGithub',\n",
       " 'RedPajamaStackExchange',\n",
       " 'RedPajamaWikipedia'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x['meta']['redpajama_set_name'] for x in test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('redpajama_set_name',)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([tuple(sorted(x['meta'].keys())) for x in test_ds['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  20204,    922,  ...,    889,    374,  13176],\n",
       "         [128000,  75341,  42691,  ..., 133333, 133333, 133333],\n",
       "         [128000,   1757,    865,  ..., 133333, 133333, 133333],\n",
       "         [128000,  27798,  46733,  ..., 133333, 133333, 133333]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pad_idx, max_len = 133333, 2048\n",
    "    batch_max = min(max_len, max(map(lambda x: len(x['input_ids']), batch)))\n",
    "\n",
    "    ret = dict()\n",
    "    ret['input_ids'] = torch.tensor([\n",
    "        x['input_ids'][:batch_max] + [pad_idx] * max(0, batch_max - len(x['input_ids']))\n",
    "        for x in batch\n",
    "    ])\n",
    "    ret['attention_mask'] = torch.tensor([\n",
    "        x['attention_mask'][:batch_max] + [0] * max(0, batch_max - len(x['attention_mask']))\n",
    "        for x in batch\n",
    "    ])\n",
    "    \n",
    "    return ret\n",
    "\n",
    "test_dl = DataLoader(test_ds['train'], batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Posts about 5 non negotiables dating non-negotiables. I love more than a dating after 50: 12 non-negotiables. Posted by sandy weiner in love is important. Relationship. Welcome to make a sunday afternoon of non-negotiables or external qualities that every. What Are. Your 5 Non-Negotiables. Posted by Rosie Qualtere-Burcher on October 13, 2016. A little under two years ago . Its funny to look at the list I have made for myself now, and realize that some of the men. I have dated in my past hardly met any of the criteria. Your 5 non-negotiable\\'s are things which when it comes to relationships you can\\'t stand, and in order to set an outline for what you are looking for, you need to decide what your 5 non-negotiables are. They could be anything, but make them realistic and be honest. The reason is simple, Non-negotiables are key. Core values that you must have aligned with a partner andor a relationship in order for that . When Is Tom Leake On A Dating Sites are dating DDating honesty Dating Datjng Dating Tyes Non-negotiables the lies are so Dating Typds spot 3Ds being Types. Lied Personaliyt Boutique. Not Tjpes I live in a Dating where women are far more promiscuous Games anywhere Games. The Dating with the national Datig for sexual partners for women ePrsonality 20. Personality to Boutique but this shows Pedsonality relationships with Dating women don\\'. I believe Games. The basis Types any Sim Personalihy based on Trust, Style, Companionship, someone Perxonality share a few Perosnality with. Communication is also Personaality important; Datung everything, the Dating, the Tjpes and the down Personalty ugly. Non-Negotiables Sim Discreet Dating Service Boutique Datinv Types the Incarnation. Adriel Sanchez Friday, 23 Mar 18. Many churches Style the denominational 3Ds celebrate Personality aDting of Dqting Christ around this time. Of year. Psrsonality Non-Negotiables of Professional Development. This post was originally Daating on Personxlity on Tupes. These five points Personality Tyles I Types are Style non-negotiables when planning \"Personality\" form of professional development. Relationship Goals: You don\\'t have to discuss this on your first date, but before it gets serious, you\\'ll want to figure out whether you . For more from Marni, visit her site, Dating With Dignity. Have you discussed each of these five non-negotiables with your guy. 5 non-negotiables to shopping smart. Expert fashion and shopping tips that will help you get the best return. On your wardrobe to . From countless years of dressing women, I have found that there are five non-negotiable keys to making the best wardrobe. Investments So what are the 5 non-negotiables? . These five issues are extremely important when voting this election season. These are five issues that the Catholic Church stands firm on, so, as practicing Catholics, we. Need to stand firm with her. One of the ladies in attendance asked the question: What are your Non-Negotiables?…These are the 5 things that I am unwilling to budge . So what are your 5 Non-Negotiables.\\nWhat does he believe. Learn more about each other\\'s faith. Maybe I. Would consider dating a person from one religion versus another, or maybe someone who is not fundamentally attached to their religious beliefs, but it\\'s situational. There are other aspects in which online dating leads to different results than offline dating. One is that people are more likely to date someone of another religion. High quality example. Sentences with of another religion in context from reliable sources - Ludwig is the linguistic search engine that helps you to write better in English. When dating someone belonging to other religion, you have to depend more on . Dating someone who is following a different religion is more about convincing each other. Only unions conducted according to the. Rules of officially recognised religions can be registered. In Indonesia children of unregistered unions cannot get birth certificates. Ted Vincent discusses dating someone from another religion. The Big Story: Origins of Religion - Продолжительность: 4:14 Yathish Dhavala 1 181. Persoality просмотров. Religion Perrsonality always Style the hard limit for some people; Dating it\\'s Typpes issue within your Sim family, it can be really challenging when the person Typds Games keen Sim doesn\\'t match. Dating Dating Personalitt another culture Dating something Dating can at once Dating beautiful and . Pr1M Online Dating example, if Sim have children Dtaing you Personaligy 3Ds two Som religions, Datiny how do. Why is he Style dating. Sim outside Dating religion if it,s 3Ds. Dear Evan, Types you Typez dated someone who was really . Personaloty thing SSim, Im Ttpes of any religion, but Im Personlity. Dating outside Datinng religious Persojality can be difficult. However, with an Boutique Datng and Personality for Persojality religious Personality, it is Daating to date 3Ds of. Another religion. Does Dating The Fossil Record Activity Answers someone who Pereonality of another culture Personality part Personality the world imply inadequacy? . Dating Types race, religion, culture, Sik is one\\'s Perssonality not Personality Simm. Dating Types already complex, Perwonality, Types messy, but dating someone. Of Personalify different Games adds an . So if you want to successfully date someone of a different Sim, put away. I would date someone from another. Religion but only if that person didn\\'t practice her . Religion is just a different view of something so yes. I\\'d most definitely date someone. So, share how it was dating another religion? . i dated a girl for 2 years who was pretty heavily religious. She also had kids 18 and 9 who she was obviously teaching to be good. to convert (someone) from one religious faith to another. \"Missionary\" is generally used to mean someone who has. Made converting others to his religion his life\\'s work. Meet Religious Singles and Date Someone with The Same Faith. It\\'s obvious that the ideal partner should be a soul mate - a person with common interests and outlook. Debonair another. Faith dating music saw the birth of the power ballad, as though we all someone of faith someone another faith want a friends with benefits.\\nNondrinker is. A derived term of drinker. As nouns the difference between nondrinker and drinker. is that nondrinker is one who does not. Drink alcohol; a teetotaler while drinker is agent noun of drink; someone or something that drinks. Binge drinking is a kind of alcoholism marked by heavy, episodic drinking followed by a period of non-dependence on or relatively low . About binge drinking. In order to identify your date as a binge drinker, you first need to know. What the term means and how to. Guest Post: Dating Without Drinking. Here at The Cooties Report, were Style looking for Dating perspectives and Ben Foster Dated takes on .\\nIve Games heard Games Petsonality hesitation Tjpes the thought of dating Skm non-drinker, Types I can Style. Understand Boutique Personqlity Boutique. These are the Boutique to the Sij asked questions about Joanns Phoenix Datin services and Style for Perzonality. If Tgpes complete 3Ds profile. EliteSingles\\' Tpyes is also 3Ds by how far your Dwting is. Datong, and Match.Com Dating Events. Take the time to Datinf all of the questions. Games photos, Peronality give yourself the best.\\nPedsonality Choose a Matchmaking Dating Site. Matchmaker sites are perfect for those . Profiles are often more detailed and sometimes ask philosophical. Questions that delve. Professional Matchmakers in New York City, NYC, Matchmaking Services in NYC. We ask you a few questions about who you are and what you are looking for, and in turn. Basic multiplayer. Matchmaking implementation. I am currently writing a matchmaking server application for a 5vs5 UDK game. Here is the scenario on which I built. My code. Questions of Time.\\nThere are a number of avenues of exploration, but the metrics .\\nWould you like to affiliate with us. comment this post. Yes The Glee Cast are all good singers in there own way. Because they have all been on Broadway, or had some sort of Singing Experience like Darren . Many of them had singing careers before the show, and the cast did concert tours towards the end of.\\nThe shows run. The Glee cast has been keeping busy with new projects. The original cast is pictured on May 20, 2010 in Universal City, California. Kevin McHale - Artie was an original member of the Glee cast . Mark Salling - The Puck actor doesnt have any. Post. Before Glee, McHale was a member of boy band NLT, which helped shape his musical chops for his role as Artie Abrams. One of Glee s last additions - she played Marley Rose Types the final few Datingg of the Persomality after the lead.\\nCharacters went Dating to college. Glee Tjpes 5 5 Cast Members Dating to Series Regulars Glee Season 5 Photo Shoot (PHOTOS) Glee Season 5 Four Original Cast Members Not Returning as Series Personality Typed Sim 5 Kevin McHale, Chris Colfer Dating Persnoality Sim and More. Its safe to say TTypes most Types the stars of Glee are \"Types\" normal, Types lives, and Salling who is facing . McHale portrayed Artie, Typez glee Sim member in the. Wheelchair. Dating Persoality Personality Slm dating Mumford and Types member Datimg Marshall Sim to.\\nMike Chang can dance. Better than Psrsonality of Dating glee members Personality his singing confidence was Sim. She is definitely one of the best singers on Sim and Personalityy is Dating underrated. Everybody look Muslim Speed Dating Events Leeds her like she\\'s the stupid one and like she. Has nothing. Meet the cast and learn more about the stars of of Glee with exclusive news, photos, videos and more at TVGuide. com. A \"High School Musical\" with bite created by Ryan Murphy (\"NipTuck\"), \"Glee\".\\nWas one of the breakout hits of Fall 2009 and ran for five. Glees two-part series finale airs Friday, but it wont be the first Glee episode to feel like a true conclusion. With the rumored return of almost all of Glees original cast. Members, the actual series finale promises to be pretty epic, but here are seven of the shows. Another Glee Cast Member. I dont know if you know this, but everyone on Glee seems .\\nThere are all kinds of weird rumors about Lea Michele dating a gigolo.',\n",
       " 'meta': {'redpajama_set_name': 'RedPajamaC4'},\n",
       " 'input_ids': [128000,\n",
       "  20204,\n",
       "  922,\n",
       "  220,\n",
       "  20,\n",
       "  2536,\n",
       "  11903,\n",
       "  2205,\n",
       "  82,\n",
       "  5029,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  13,\n",
       "  358,\n",
       "  3021,\n",
       "  810,\n",
       "  1109,\n",
       "  264,\n",
       "  5029,\n",
       "  1306,\n",
       "  220,\n",
       "  1135,\n",
       "  25,\n",
       "  220,\n",
       "  717,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  13,\n",
       "  15634,\n",
       "  555,\n",
       "  68539,\n",
       "  584,\n",
       "  10670,\n",
       "  304,\n",
       "  3021,\n",
       "  374,\n",
       "  3062,\n",
       "  13,\n",
       "  33907,\n",
       "  13,\n",
       "  20776,\n",
       "  311,\n",
       "  1304,\n",
       "  264,\n",
       "  93463,\n",
       "  13658,\n",
       "  315,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  477,\n",
       "  9434,\n",
       "  29600,\n",
       "  430,\n",
       "  1475,\n",
       "  13,\n",
       "  3639,\n",
       "  8886,\n",
       "  13,\n",
       "  4718,\n",
       "  220,\n",
       "  20,\n",
       "  11842,\n",
       "  11500,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  13,\n",
       "  15634,\n",
       "  555,\n",
       "  97867,\n",
       "  20143,\n",
       "  55704,\n",
       "  7826,\n",
       "  2639,\n",
       "  261,\n",
       "  389,\n",
       "  6664,\n",
       "  220,\n",
       "  1032,\n",
       "  11,\n",
       "  220,\n",
       "  679,\n",
       "  21,\n",
       "  13,\n",
       "  362,\n",
       "  2697,\n",
       "  1234,\n",
       "  1403,\n",
       "  1667,\n",
       "  4227,\n",
       "  662,\n",
       "  11699,\n",
       "  15526,\n",
       "  311,\n",
       "  1427,\n",
       "  520,\n",
       "  279,\n",
       "  1160,\n",
       "  358,\n",
       "  617,\n",
       "  1903,\n",
       "  369,\n",
       "  7182,\n",
       "  1457,\n",
       "  11,\n",
       "  323,\n",
       "  13383,\n",
       "  430,\n",
       "  1063,\n",
       "  315,\n",
       "  279,\n",
       "  3026,\n",
       "  13,\n",
       "  358,\n",
       "  617,\n",
       "  30105,\n",
       "  304,\n",
       "  856,\n",
       "  3347,\n",
       "  20781,\n",
       "  2322,\n",
       "  904,\n",
       "  315,\n",
       "  279,\n",
       "  13186,\n",
       "  13,\n",
       "  4718,\n",
       "  220,\n",
       "  20,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  596,\n",
       "  527,\n",
       "  2574,\n",
       "  902,\n",
       "  994,\n",
       "  433,\n",
       "  4131,\n",
       "  311,\n",
       "  12135,\n",
       "  499,\n",
       "  649,\n",
       "  956,\n",
       "  2559,\n",
       "  11,\n",
       "  323,\n",
       "  304,\n",
       "  2015,\n",
       "  311,\n",
       "  743,\n",
       "  459,\n",
       "  21782,\n",
       "  369,\n",
       "  1148,\n",
       "  499,\n",
       "  527,\n",
       "  3411,\n",
       "  369,\n",
       "  11,\n",
       "  499,\n",
       "  1205,\n",
       "  311,\n",
       "  10491,\n",
       "  1148,\n",
       "  701,\n",
       "  220,\n",
       "  20,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  527,\n",
       "  13,\n",
       "  2435,\n",
       "  1436,\n",
       "  387,\n",
       "  4205,\n",
       "  11,\n",
       "  719,\n",
       "  1304,\n",
       "  1124,\n",
       "  26569,\n",
       "  323,\n",
       "  387,\n",
       "  10978,\n",
       "  13,\n",
       "  578,\n",
       "  2944,\n",
       "  374,\n",
       "  4382,\n",
       "  11,\n",
       "  11842,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  527,\n",
       "  1401,\n",
       "  13,\n",
       "  9708,\n",
       "  2819,\n",
       "  430,\n",
       "  499,\n",
       "  2011,\n",
       "  617,\n",
       "  27210,\n",
       "  449,\n",
       "  264,\n",
       "  8427,\n",
       "  323,\n",
       "  269,\n",
       "  264,\n",
       "  5133,\n",
       "  304,\n",
       "  2015,\n",
       "  369,\n",
       "  430,\n",
       "  662,\n",
       "  3277,\n",
       "  2209,\n",
       "  8529,\n",
       "  2009,\n",
       "  731,\n",
       "  1952,\n",
       "  362,\n",
       "  17783,\n",
       "  33038,\n",
       "  527,\n",
       "  5029,\n",
       "  32004,\n",
       "  1113,\n",
       "  48948,\n",
       "  17783,\n",
       "  22362,\n",
       "  73,\n",
       "  983,\n",
       "  17783,\n",
       "  350,\n",
       "  9891,\n",
       "  11842,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  279,\n",
       "  15812,\n",
       "  527,\n",
       "  779,\n",
       "  17783,\n",
       "  17977,\n",
       "  5469,\n",
       "  7858,\n",
       "  220,\n",
       "  18,\n",
       "  68050,\n",
       "  1694,\n",
       "  21431,\n",
       "  13,\n",
       "  445,\n",
       "  1142,\n",
       "  7508,\n",
       "  8115,\n",
       "  16820,\n",
       "  91217,\n",
       "  13,\n",
       "  2876,\n",
       "  350,\n",
       "  73,\n",
       "  20975,\n",
       "  358,\n",
       "  3974,\n",
       "  304,\n",
       "  264,\n",
       "  17783,\n",
       "  1405,\n",
       "  3278,\n",
       "  527,\n",
       "  3117,\n",
       "  810,\n",
       "  2773,\n",
       "  3510,\n",
       "  9373,\n",
       "  11871,\n",
       "  12660,\n",
       "  11871,\n",
       "  13,\n",
       "  578,\n",
       "  17783,\n",
       "  449,\n",
       "  279,\n",
       "  5426,\n",
       "  22362,\n",
       "  343,\n",
       "  369,\n",
       "  7392,\n",
       "  8717,\n",
       "  369,\n",
       "  3278,\n",
       "  384,\n",
       "  3617,\n",
       "  942,\n",
       "  2786,\n",
       "  220,\n",
       "  508,\n",
       "  13,\n",
       "  65915,\n",
       "  311,\n",
       "  91217,\n",
       "  719,\n",
       "  420,\n",
       "  5039,\n",
       "  19878,\n",
       "  942,\n",
       "  2786,\n",
       "  12135,\n",
       "  449,\n",
       "  17783,\n",
       "  3278,\n",
       "  1541,\n",
       "  4527,\n",
       "  358,\n",
       "  4510,\n",
       "  11871,\n",
       "  13,\n",
       "  578,\n",
       "  8197,\n",
       "  21431,\n",
       "  904,\n",
       "  4567,\n",
       "  19758,\n",
       "  7141,\n",
       "  88,\n",
       "  3196,\n",
       "  389,\n",
       "  17236,\n",
       "  11,\n",
       "  12179,\n",
       "  11,\n",
       "  1219,\n",
       "  857,\n",
       "  919,\n",
       "  2200,\n",
       "  11,\n",
       "  4423,\n",
       "  3700,\n",
       "  62585,\n",
       "  2786,\n",
       "  4430,\n",
       "  264,\n",
       "  2478,\n",
       "  3700,\n",
       "  437,\n",
       "  77,\n",
       "  2786,\n",
       "  449,\n",
       "  13,\n",
       "  31966,\n",
       "  374,\n",
       "  1101,\n",
       "  54990,\n",
       "  2786,\n",
       "  3062,\n",
       "  26,\n",
       "  22362,\n",
       "  2234,\n",
       "  4395,\n",
       "  11,\n",
       "  279,\n",
       "  17783,\n",
       "  11,\n",
       "  279,\n",
       "  350,\n",
       "  73,\n",
       "  20975,\n",
       "  323,\n",
       "  279,\n",
       "  1523,\n",
       "  7508,\n",
       "  10231,\n",
       "  28360,\n",
       "  13,\n",
       "  11842,\n",
       "  11500,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  4567,\n",
       "  4185,\n",
       "  846,\n",
       "  295,\n",
       "  17783,\n",
       "  5475,\n",
       "  91217,\n",
       "  22362,\n",
       "  14386,\n",
       "  21431,\n",
       "  279,\n",
       "  4953,\n",
       "  55149,\n",
       "  13,\n",
       "  2467,\n",
       "  22811,\n",
       "  48254,\n",
       "  6740,\n",
       "  11,\n",
       "  220,\n",
       "  1419,\n",
       "  2947,\n",
       "  220,\n",
       "  972,\n",
       "  13,\n",
       "  9176,\n",
       "  31012,\n",
       "  12179,\n",
       "  279,\n",
       "  62265,\n",
       "  1697,\n",
       "  220,\n",
       "  18,\n",
       "  68050,\n",
       "  18890,\n",
       "  65915,\n",
       "  264,\n",
       "  35,\n",
       "  1303,\n",
       "  315,\n",
       "  423,\n",
       "  80,\n",
       "  1303,\n",
       "  3771,\n",
       "  2212,\n",
       "  420,\n",
       "  892,\n",
       "  13,\n",
       "  5046,\n",
       "  1060,\n",
       "  13,\n",
       "  90522,\n",
       "  942,\n",
       "  2786,\n",
       "  11842,\n",
       "  11500,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  315,\n",
       "  21931,\n",
       "  11050,\n",
       "  13,\n",
       "  1115,\n",
       "  1772,\n",
       "  574,\n",
       "  13517,\n",
       "  14569,\n",
       "  1113,\n",
       "  389,\n",
       "  7508,\n",
       "  25299,\n",
       "  488,\n",
       "  389,\n",
       "  97197,\n",
       "  288,\n",
       "  13,\n",
       "  4314,\n",
       "  4330,\n",
       "  3585,\n",
       "  65915,\n",
       "  350,\n",
       "  2552,\n",
       "  358,\n",
       "  21431,\n",
       "  527,\n",
       "  12179,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  994,\n",
       "  9293,\n",
       "  330,\n",
       "  10909,\n",
       "  2786,\n",
       "  1,\n",
       "  1376,\n",
       "  315,\n",
       "  6721,\n",
       "  4500,\n",
       "  13,\n",
       "  33907,\n",
       "  55293,\n",
       "  25,\n",
       "  1472,\n",
       "  1541,\n",
       "  956,\n",
       "  617,\n",
       "  311,\n",
       "  4358,\n",
       "  420,\n",
       "  389,\n",
       "  701,\n",
       "  1176,\n",
       "  2457,\n",
       "  11,\n",
       "  719,\n",
       "  1603,\n",
       "  433,\n",
       "  5334,\n",
       "  6129,\n",
       "  11,\n",
       "  499,\n",
       "  3358,\n",
       "  1390,\n",
       "  311,\n",
       "  7216,\n",
       "  704,\n",
       "  3508,\n",
       "  499,\n",
       "  662,\n",
       "  1789,\n",
       "  810,\n",
       "  505,\n",
       "  386,\n",
       "  1923,\n",
       "  72,\n",
       "  11,\n",
       "  4034,\n",
       "  1077,\n",
       "  2816,\n",
       "  11,\n",
       "  17783,\n",
       "  3161,\n",
       "  423,\n",
       "  625,\n",
       "  488,\n",
       "  13,\n",
       "  12522,\n",
       "  499,\n",
       "  14407,\n",
       "  1855,\n",
       "  315,\n",
       "  1521,\n",
       "  4330,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  449,\n",
       "  701,\n",
       "  7564,\n",
       "  13,\n",
       "  220,\n",
       "  20,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  311,\n",
       "  12185,\n",
       "  7941,\n",
       "  13,\n",
       "  33257,\n",
       "  11401,\n",
       "  323,\n",
       "  12185,\n",
       "  10631,\n",
       "  430,\n",
       "  690,\n",
       "  1520,\n",
       "  499,\n",
       "  636,\n",
       "  279,\n",
       "  1888,\n",
       "  471,\n",
       "  13,\n",
       "  1952,\n",
       "  701,\n",
       "  46846,\n",
       "  311,\n",
       "  662,\n",
       "  5659,\n",
       "  28701,\n",
       "  1667,\n",
       "  315,\n",
       "  32623,\n",
       "  3278,\n",
       "  11,\n",
       "  358,\n",
       "  617,\n",
       "  1766,\n",
       "  430,\n",
       "  1070,\n",
       "  527,\n",
       "  4330,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  7039,\n",
       "  311,\n",
       "  3339,\n",
       "  279,\n",
       "  1888,\n",
       "  46846,\n",
       "  13,\n",
       "  77990,\n",
       "  2100,\n",
       "  1148,\n",
       "  527,\n",
       "  279,\n",
       "  220,\n",
       "  20,\n",
       "  2536,\n",
       "  5392,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  30,\n",
       "  662,\n",
       "  4314,\n",
       "  4330,\n",
       "  4819,\n",
       "  527,\n",
       "  9193,\n",
       "  3062,\n",
       "  994,\n",
       "  16043,\n",
       "  420,\n",
       "  6355,\n",
       "  3280,\n",
       "  13,\n",
       "  4314,\n",
       "  527,\n",
       "  4330,\n",
       "  4819,\n",
       "  430,\n",
       "  279,\n",
       "  16879,\n",
       "  9441,\n",
       "  13656,\n",
       "  7626,\n",
       "  389,\n",
       "  11,\n",
       "  779,\n",
       "  11,\n",
       "  439,\n",
       "  36666,\n",
       "  51971,\n",
       "  11,\n",
       "  584,\n",
       "  13,\n",
       "  14998,\n",
       "  311,\n",
       "  2559,\n",
       "  7626,\n",
       "  449,\n",
       "  1077,\n",
       "  13,\n",
       "  3861,\n",
       "  315,\n",
       "  279,\n",
       "  23628,\n",
       "  304,\n",
       "  28116,\n",
       "  4691,\n",
       "  279,\n",
       "  3488,\n",
       "  25,\n",
       "  3639,\n",
       "  527,\n",
       "  701,\n",
       "  11842,\n",
       "  11500,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  30,\n",
       "  1981,\n",
       "  9673,\n",
       "  527,\n",
       "  279,\n",
       "  220,\n",
       "  20,\n",
       "  2574,\n",
       "  430,\n",
       "  358,\n",
       "  1097,\n",
       "  46243,\n",
       "  311,\n",
       "  293,\n",
       "  20132,\n",
       "  662,\n",
       "  2100,\n",
       "  1148,\n",
       "  527,\n",
       "  701,\n",
       "  220,\n",
       "  20,\n",
       "  11842,\n",
       "  11500,\n",
       "  67078,\n",
       "  2205,\n",
       "  82,\n",
       "  627,\n",
       "  3923,\n",
       "  1587,\n",
       "  568,\n",
       "  4510,\n",
       "  13,\n",
       "  15281,\n",
       "  810,\n",
       "  922,\n",
       "  1855,\n",
       "  1023,\n",
       "  596,\n",
       "  10082,\n",
       "  13,\n",
       "  10926,\n",
       "  358,\n",
       "  13,\n",
       "  19418,\n",
       "  2980,\n",
       "  5029,\n",
       "  264,\n",
       "  1732,\n",
       "  505,\n",
       "  832,\n",
       "  13901,\n",
       "  19579,\n",
       "  2500,\n",
       "  11,\n",
       "  477,\n",
       "  7344,\n",
       "  4423,\n",
       "  889,\n",
       "  374,\n",
       "  539,\n",
       "  43593,\n",
       "  12673,\n",
       "  311,\n",
       "  872,\n",
       "  10597,\n",
       "  21463,\n",
       "  11,\n",
       "  719,\n",
       "  433,\n",
       "  596,\n",
       "  10109,\n",
       "  1697,\n",
       "  13,\n",
       "  2684,\n",
       "  527,\n",
       "  1023,\n",
       "  13878,\n",
       "  304,\n",
       "  902,\n",
       "  2930,\n",
       "  5029,\n",
       "  11767,\n",
       "  311,\n",
       "  2204,\n",
       "  3135,\n",
       "  1109,\n",
       "  27258,\n",
       "  5029,\n",
       "  13,\n",
       "  3861,\n",
       "  374,\n",
       "  430,\n",
       "  1274,\n",
       "  527,\n",
       "  810,\n",
       "  4461,\n",
       "  311,\n",
       "  2457,\n",
       "  4423,\n",
       "  315,\n",
       "  2500,\n",
       "  13901,\n",
       "  13,\n",
       "  5234,\n",
       "  4367,\n",
       "  3187,\n",
       "  13,\n",
       "  24248,\n",
       "  2436,\n",
       "  449,\n",
       "  315,\n",
       "  2500,\n",
       "  13901,\n",
       "  304,\n",
       "  2317,\n",
       "  505,\n",
       "  15062,\n",
       "  8336,\n",
       "  482,\n",
       "  85550,\n",
       "  374,\n",
       "  279,\n",
       "  65767,\n",
       "  2778,\n",
       "  4817,\n",
       "  430,\n",
       "  8779,\n",
       "  499,\n",
       "  311,\n",
       "  3350,\n",
       "  2731,\n",
       "  304,\n",
       "  6498,\n",
       "  13,\n",
       "  3277,\n",
       "  5029,\n",
       "  4423,\n",
       "  33152,\n",
       "  311,\n",
       "  1023,\n",
       "  13901,\n",
       "  11,\n",
       "  499,\n",
       "  617,\n",
       "  311,\n",
       "  6904,\n",
       "  810,\n",
       "  389,\n",
       "  662,\n",
       "  17783,\n",
       "  4423,\n",
       "  889,\n",
       "  374,\n",
       "  2768,\n",
       "  264,\n",
       "  2204,\n",
       "  13901,\n",
       "  374,\n",
       "  810,\n",
       "  922,\n",
       "  40661,\n",
       "  1855,\n",
       "  1023,\n",
       "  13,\n",
       "  8442,\n",
       "  29590,\n",
       "  13375,\n",
       "  4184,\n",
       "  311,\n",
       "  279,\n",
       "  13,\n",
       "  23694,\n",
       "  315,\n",
       "  19073,\n",
       "  39764,\n",
       "  44230,\n",
       "  649,\n",
       "  387,\n",
       "  9879,\n",
       "  13,\n",
       "  763,\n",
       "  24922,\n",
       "  2911,\n",
       "  315,\n",
       "  653,\n",
       "  36009,\n",
       "  29590,\n",
       "  4250,\n",
       "  636,\n",
       "  7342,\n",
       "  35537,\n",
       "  13,\n",
       "  23989,\n",
       "  35407,\n",
       "  35434,\n",
       "  5029,\n",
       "  4423,\n",
       "  505,\n",
       "  2500,\n",
       "  13901,\n",
       "  13,\n",
       "  578,\n",
       "  6295,\n",
       "  15457,\n",
       "  25,\n",
       "  70591,\n",
       "  315,\n",
       "  44193,\n",
       "  482,\n",
       "  124630,\n",
       "  48181,\n",
       "  126188,\n",
       "  25,\n",
       "  220,\n",
       "  19,\n",
       "  25,\n",
       "  975,\n",
       "  816,\n",
       "  589,\n",
       "  819,\n",
       "  44327,\n",
       "  402,\n",
       "  6181,\n",
       "  220,\n",
       "  16,\n",
       "  220,\n",
       "  10562,\n",
       "  13,\n",
       "  21097,\n",
       "  78,\n",
       "  2786,\n",
       "  106427,\n",
       "  82224,\n",
       "  24803,\n",
       "  13,\n",
       "  44193,\n",
       "  86848,\n",
       "  942,\n",
       "  2786,\n",
       "  2744,\n",
       "  12179,\n",
       "  279,\n",
       "  2653,\n",
       "  4017,\n",
       "  369,\n",
       "  1063,\n",
       "  1274,\n",
       "  26,\n",
       "  17783,\n",
       "  433,\n",
       "  596,\n",
       "  14221,\n",
       "  604,\n",
       "  288,\n",
       "  4360,\n",
       "  2949,\n",
       "  701,\n",
       "  4567,\n",
       "  3070,\n",
       "  11,\n",
       "  433,\n",
       "  649,\n",
       "  387,\n",
       "  2216,\n",
       "  17436,\n",
       "  994,\n",
       "  279,\n",
       "  1732,\n",
       "  17977,\n",
       "  5469,\n",
       "  11871,\n",
       "  27989,\n",
       "  4567,\n",
       "  3250,\n",
       "  956,\n",
       "  2489,\n",
       "  13,\n",
       "  17783,\n",
       "  17783,\n",
       "  19758,\n",
       "  1468,\n",
       "  2500,\n",
       "  7829,\n",
       "  17783,\n",
       "  2555,\n",
       "  17783,\n",
       "  649,\n",
       "  520,\n",
       "  3131,\n",
       "  17783,\n",
       "  6366,\n",
       "  323,\n",
       "  662,\n",
       "  2394,\n",
       "  16,\n",
       "  44,\n",
       "  8267,\n",
       "  17783,\n",
       "  3187,\n",
       "  11,\n",
       "  422,\n",
       "  4567,\n",
       "  617,\n",
       "  2911,\n",
       "  423,\n",
       "  2629,\n",
       "  287,\n",
       "  499,\n",
       "  19758,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(inmemory_300k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_padding(batch, pad_idx, max_len=2048):\n",
    "    batch_max = min(max_len, max(map(len, batch['input_ids'])))\n",
    "\n",
    "    batch['input_ids'] = torch.tensor([\n",
    "        x[:batch_max] + [pad_idx] * max(0, batch_max - len(x))\n",
    "        for x in batch['input_ids']\n",
    "    ])\n",
    "    batch['attention_mask'] = torch.tensor([\n",
    "        x[:batch_max] + [0] * max(0, batch_max - len(x))\n",
    "        for x in batch['attention_mask']\n",
    "    ])\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset = load_dataset('cerebras/SlimPajama-627B', split=f\"train\", streaming=True)\n",
    "dataset = (dataset\n",
    "    .shuffle(seed=42, buffer_size=10_000)\n",
    "    .map(lambda x: tokenizer(x['text']), batched=True, batch_size=100, remove_columns=['text', 'meta'])\n",
    "        # .with_format(\"torch\")\n",
    "\n",
    "    .batch(2)\n",
    "    .map(lambda x: batch_padding(x, tokenizer.pad_token_id, 2048))\n",
    ")\n",
    "\n",
    "# dataset = dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch['input_ids'][0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   4599,   1053,    499,   1093,    311,   4822,    520,    350,\n",
       "           45036,  18072,  59359,    527,  19167,    449,    264,   6007,    323,\n",
       "             665,  16578,  13077,    627,   2028,   3130,   6209,    264,   1684,\n",
       "             315,    279,  24269,   7463,    323,   5764,    264,    879,  15197,\n",
       "             449,    264,  81188,    627,   2028,   2033,   3130,    706,    264,\n",
       "           10228,   3262,    277,    901,   6558,    323,   3805,  35121,  82927,\n",
       "            5296,    430,  14177,    374,   5343,    389,    279,   1938,    315,\n",
       "           19163,     11,    323,  17954,    374,   5343,    279,   1828,   1938,\n",
       "              13,  49191,  17610,    315,    264,  36433,     11,    743,   5130,\n",
       "            1701,   7878,     11,   2254,   8356,    320,   3696,  15872,    527,\n",
       "             539,   5343,   4390,   2028,  28497,   3130,    706,    264,  10228,\n",
       "            3262,    277,    901,   6558,    323,   3805,  35121,  82927,   5296,\n",
       "             430,  14177,    374,   5343,    389,    279,   1938,    315,  19163,\n",
       "              11,    323,  17954,    374,   5343,    279,   1828,   1938,     13,\n",
       "           49191,  17610,    315,    264,  36433,     11,    743,   5130,   1701,\n",
       "            7878,     11,   2254,   8356,    320,   3696,  15872,    527,    539,\n",
       "            5343,   4390,   2028,   2033,   3130,    706,    264,  10228,   3262,\n",
       "             277,    901,   6558,    323,   3805,  35121,     13,   1102,   6209,\n",
       "            2680,    311,    279,  83665,    309,    323,   4106,  15286,    627,\n",
       "             791,  14356,    374,  19167,    449,    264,   9979,   6672,    627,\n",
       "            6540,     64,   6458,    616,   2136,  12541,    374,   8036,    505,\n",
       "            6658,    220,   1721,   3297,    220,    679,     24,   3156,  36992,\n",
       "             220,   2148,   5020,    220,    679,     24,  75800,   7463,    374,\n",
       "            8036,    505,   6658,    220,   1721,   3297,    220,    679,     24,\n",
       "            3156,  36992,    220,   2148,   5020,    220,    679,     24,   1789,\n",
       "            4868,   8125,     11,   5066,  28036,    323,   8945,    272,   2469,\n",
       "             527,    539,   5535,    304,    682,  12295,  16853,   5618,   5296,\n",
       "             430,   8602,  14295,  31626,   3095,  13560,  73080,    527,    459,\n",
       "           11928,   1749,    315,   8323,     13,   5321,   5296,    430,   3091,\n",
       "           14295,    527,    539,  11928,    627,   5618,   5296,    279,   4106,\n",
       "           15286,    374,   1949,    323,    374,   1825,    505,    220,     16,\n",
       "            3297,   3156,    220,    966,   6250,     11,    719,  34350,    304,\n",
       "            1162,    315,  11422,     13,   1102,    374,  15987,    505,    220,\n",
       "             975,     25,    410,    311,    220,    508,     25,    410,    627,\n",
       "            5618,   5296,    279,  16166,   7463,    374,    539,  32813,    627,\n",
       "           23340,   9533,    374,  10434,   1475,   1938,    505,    220,   2705,\n",
       "              25,    410,    311,    220,    605,     25,    966,    627,   5618,\n",
       "            5296,    430,  10960,   8736,   3115,   1253,  13592,     13,   5321,\n",
       "            3729,    279,   9689,   6089,    369,   4726,   3649,    627,   4959,\n",
       "            1938,    584,   3358,   1817,   7729,    323,   3708,    499,    459,\n",
       "            2613,    369,    701,   4183,  13003,    520,    350,  45036,     13,\n",
       "            1442,    499,   1541,    956,    617,   3230,  13003,    719,   1053,\n",
       "            1093,    311,   1817,   7729,    369,   2019,   1828,   9178,    477,\n",
       "            2019,   1828,   2305,    584,    649,   1817,    279,   3430,   2288,\n",
       "             627,   2677,    311,   3350,    264,   3477,    323,   4430,    701,\n",
       "            3217,    315,    350,  45036,    449,   1023,  60068,     13],\n",
       "         [128000,     44,    954,   8184,   6150,    596,  11376,    374,    311,\n",
       "           12178,    279,   1917,    596,  44202,     13,   2057,  12178,    279,\n",
       "            1917,    596,  44202,   3445,   8405,    264,   2731,   3938,    369,\n",
       "           22540,    889,   1833,     11,   2737,   4367,    315,   2324,     11,\n",
       "            4676,    323,   7100,   6067,     13,   2057,  21054,    420,  11376,\n",
       "              11,   1057,   9131,    374,    311,    387,    264,  34076,   6975,\n",
       "            7471,    430,  11705,  98990,   6677,    323,  39671,  18475,    278,\n",
       "            6164,     13, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "          128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch = next(iter(dataset))\n",
    "one_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  1,   2,   3, 111, 111],\n",
       "         [  1,   2,   3,   4,   5]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_padding(\n",
    "    {\n",
    "        \"input_ids\": [[1, 2, 3], [1, 2, 3, 4, 5, 6]],\n",
    "        \"attention_mask\": [[1, 1, 1], [1,1,1,1,1,1]]\n",
    "    },\n",
    "    111,\n",
    "    5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "\n",
    "for i, x in enumerate(dataset):\n",
    "    lens.append(len(x['input_ids']))\n",
    "    \n",
    "    if i > 200: break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(707.0297029702971,\n",
       " 27,\n",
       " 9435,\n",
       " 423.0,\n",
       " 757.75,\n",
       " 2355.2499999999995,\n",
       " 4685.130000000007)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(lens), np.min(lens), np.max(lens), np.median(lens), np.percentile(lens, 75), np.percentile(lens, 95), np.percentile(lens, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000,\n",
       "  4599,\n",
       "  1053,\n",
       "  499,\n",
       "  1093,\n",
       "  311,\n",
       "  4822,\n",
       "  520,\n",
       "  350,\n",
       "  45036,\n",
       "  18072,\n",
       "  59359,\n",
       "  527,\n",
       "  19167,\n",
       "  449,\n",
       "  264,\n",
       "  6007,\n",
       "  323,\n",
       "  665,\n",
       "  16578,\n",
       "  13077,\n",
       "  627,\n",
       "  2028,\n",
       "  3130,\n",
       "  6209,\n",
       "  264,\n",
       "  1684,\n",
       "  315,\n",
       "  279,\n",
       "  24269,\n",
       "  7463,\n",
       "  323,\n",
       "  5764,\n",
       "  264,\n",
       "  879,\n",
       "  15197,\n",
       "  449,\n",
       "  264,\n",
       "  81188,\n",
       "  627,\n",
       "  2028,\n",
       "  2033,\n",
       "  3130,\n",
       "  706,\n",
       "  264,\n",
       "  10228,\n",
       "  3262,\n",
       "  277,\n",
       "  901,\n",
       "  6558,\n",
       "  323,\n",
       "  3805,\n",
       "  35121,\n",
       "  82927,\n",
       "  5296,\n",
       "  430,\n",
       "  14177,\n",
       "  374,\n",
       "  5343,\n",
       "  389,\n",
       "  279,\n",
       "  1938,\n",
       "  315,\n",
       "  19163,\n",
       "  11,\n",
       "  323,\n",
       "  17954,\n",
       "  374,\n",
       "  5343,\n",
       "  279,\n",
       "  1828,\n",
       "  1938,\n",
       "  13,\n",
       "  49191,\n",
       "  17610,\n",
       "  315,\n",
       "  264,\n",
       "  36433,\n",
       "  11,\n",
       "  743,\n",
       "  5130,\n",
       "  1701,\n",
       "  7878,\n",
       "  11,\n",
       "  2254,\n",
       "  8356,\n",
       "  320,\n",
       "  3696,\n",
       "  15872,\n",
       "  527,\n",
       "  539,\n",
       "  5343,\n",
       "  4390,\n",
       "  2028,\n",
       "  28497,\n",
       "  3130,\n",
       "  706,\n",
       "  264,\n",
       "  10228,\n",
       "  3262,\n",
       "  277,\n",
       "  901,\n",
       "  6558,\n",
       "  323,\n",
       "  3805,\n",
       "  35121,\n",
       "  82927,\n",
       "  5296,\n",
       "  430,\n",
       "  14177,\n",
       "  374,\n",
       "  5343,\n",
       "  389,\n",
       "  279,\n",
       "  1938,\n",
       "  315,\n",
       "  19163,\n",
       "  11,\n",
       "  323,\n",
       "  17954,\n",
       "  374,\n",
       "  5343,\n",
       "  279,\n",
       "  1828,\n",
       "  1938,\n",
       "  13,\n",
       "  49191,\n",
       "  17610,\n",
       "  315,\n",
       "  264,\n",
       "  36433,\n",
       "  11,\n",
       "  743,\n",
       "  5130,\n",
       "  1701,\n",
       "  7878,\n",
       "  11,\n",
       "  2254,\n",
       "  8356,\n",
       "  320,\n",
       "  3696,\n",
       "  15872,\n",
       "  527,\n",
       "  539,\n",
       "  5343,\n",
       "  4390,\n",
       "  2028,\n",
       "  2033,\n",
       "  3130,\n",
       "  706,\n",
       "  264,\n",
       "  10228,\n",
       "  3262,\n",
       "  277,\n",
       "  901,\n",
       "  6558,\n",
       "  323,\n",
       "  3805,\n",
       "  35121,\n",
       "  13,\n",
       "  1102,\n",
       "  6209,\n",
       "  2680,\n",
       "  311,\n",
       "  279,\n",
       "  83665,\n",
       "  309,\n",
       "  323,\n",
       "  4106,\n",
       "  15286,\n",
       "  627,\n",
       "  791,\n",
       "  14356,\n",
       "  374,\n",
       "  19167,\n",
       "  449,\n",
       "  264,\n",
       "  9979,\n",
       "  6672,\n",
       "  627,\n",
       "  6540,\n",
       "  64,\n",
       "  6458,\n",
       "  616,\n",
       "  2136,\n",
       "  12541,\n",
       "  374,\n",
       "  8036,\n",
       "  505,\n",
       "  6658,\n",
       "  220,\n",
       "  1721,\n",
       "  3297,\n",
       "  220,\n",
       "  679,\n",
       "  24,\n",
       "  3156,\n",
       "  36992,\n",
       "  220,\n",
       "  2148,\n",
       "  5020,\n",
       "  220,\n",
       "  679,\n",
       "  24,\n",
       "  75800,\n",
       "  7463,\n",
       "  374,\n",
       "  8036,\n",
       "  505,\n",
       "  6658,\n",
       "  220,\n",
       "  1721,\n",
       "  3297,\n",
       "  220,\n",
       "  679,\n",
       "  24,\n",
       "  3156,\n",
       "  36992,\n",
       "  220,\n",
       "  2148,\n",
       "  5020,\n",
       "  220,\n",
       "  679,\n",
       "  24,\n",
       "  1789,\n",
       "  4868,\n",
       "  8125,\n",
       "  11,\n",
       "  5066,\n",
       "  28036,\n",
       "  323,\n",
       "  8945,\n",
       "  272,\n",
       "  2469,\n",
       "  527,\n",
       "  539,\n",
       "  5535,\n",
       "  304,\n",
       "  682,\n",
       "  12295,\n",
       "  16853,\n",
       "  5618,\n",
       "  5296,\n",
       "  430,\n",
       "  8602,\n",
       "  14295,\n",
       "  31626,\n",
       "  3095,\n",
       "  13560,\n",
       "  73080,\n",
       "  527,\n",
       "  459,\n",
       "  11928,\n",
       "  1749,\n",
       "  315,\n",
       "  8323,\n",
       "  13,\n",
       "  5321,\n",
       "  5296,\n",
       "  430,\n",
       "  3091,\n",
       "  14295,\n",
       "  527,\n",
       "  539,\n",
       "  11928,\n",
       "  627,\n",
       "  5618,\n",
       "  5296,\n",
       "  279,\n",
       "  4106,\n",
       "  15286,\n",
       "  374,\n",
       "  1949,\n",
       "  323,\n",
       "  374,\n",
       "  1825,\n",
       "  505,\n",
       "  220,\n",
       "  16,\n",
       "  3297,\n",
       "  3156,\n",
       "  220,\n",
       "  966,\n",
       "  6250,\n",
       "  11,\n",
       "  719,\n",
       "  34350,\n",
       "  304,\n",
       "  1162,\n",
       "  315,\n",
       "  11422,\n",
       "  13,\n",
       "  1102,\n",
       "  374,\n",
       "  15987,\n",
       "  505,\n",
       "  220,\n",
       "  975,\n",
       "  25,\n",
       "  410,\n",
       "  311,\n",
       "  220,\n",
       "  508,\n",
       "  25,\n",
       "  410,\n",
       "  627,\n",
       "  5618,\n",
       "  5296,\n",
       "  279,\n",
       "  16166,\n",
       "  7463,\n",
       "  374,\n",
       "  539,\n",
       "  32813,\n",
       "  627,\n",
       "  23340,\n",
       "  9533,\n",
       "  374,\n",
       "  10434,\n",
       "  1475,\n",
       "  1938,\n",
       "  505,\n",
       "  220,\n",
       "  2705,\n",
       "  25,\n",
       "  410,\n",
       "  311,\n",
       "  220,\n",
       "  605,\n",
       "  25,\n",
       "  966,\n",
       "  627,\n",
       "  5618,\n",
       "  5296,\n",
       "  430,\n",
       "  10960,\n",
       "  8736,\n",
       "  3115,\n",
       "  1253,\n",
       "  13592,\n",
       "  13,\n",
       "  5321,\n",
       "  3729,\n",
       "  279,\n",
       "  9689,\n",
       "  6089,\n",
       "  369,\n",
       "  4726,\n",
       "  3649,\n",
       "  627,\n",
       "  4959,\n",
       "  1938,\n",
       "  584,\n",
       "  3358,\n",
       "  1817,\n",
       "  7729,\n",
       "  323,\n",
       "  3708,\n",
       "  499,\n",
       "  459,\n",
       "  2613,\n",
       "  369,\n",
       "  701,\n",
       "  4183,\n",
       "  13003,\n",
       "  520,\n",
       "  350,\n",
       "  45036,\n",
       "  13,\n",
       "  1442,\n",
       "  499,\n",
       "  1541,\n",
       "  956,\n",
       "  617,\n",
       "  3230,\n",
       "  13003,\n",
       "  719,\n",
       "  1053,\n",
       "  1093,\n",
       "  311,\n",
       "  1817,\n",
       "  7729,\n",
       "  369,\n",
       "  2019,\n",
       "  1828,\n",
       "  9178,\n",
       "  477,\n",
       "  2019,\n",
       "  1828,\n",
       "  2305,\n",
       "  584,\n",
       "  649,\n",
       "  1817,\n",
       "  279,\n",
       "  3430,\n",
       "  2288,\n",
       "  627,\n",
       "  2677,\n",
       "  311,\n",
       "  3350,\n",
       "  264,\n",
       "  3477,\n",
       "  323,\n",
       "  4430,\n",
       "  701,\n",
       "  3217,\n",
       "  315,\n",
       "  350,\n",
       "  45036,\n",
       "  449,\n",
       "  1023,\n",
       "  60068,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_batch = next(iter(dataset))\n",
    "one_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdpa'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config._attn_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 23.69 GiB of which 15.91 GiB is free. Process 858019 has 934.00 MiB memory in use. Process 859749 has 934.00 MiB memory in use. Including non-PyTorch memory, this process has 5.92 GiB memory in use. Of the allocated memory 5.63 GiB is allocated by PyTorch, and 1003.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:541\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    539\u001b[39m     position_ids = cache_position.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m causal_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m hidden_states = inputs_embeds\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:647\u001b[39m, in \u001b[36mLlamaModel._update_causal_mask\u001b[39m\u001b[34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[39m\n\u001b[32m    640\u001b[39m     target_length = (\n\u001b[32m    641\u001b[39m         attention_mask.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    642\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_mask, torch.Tensor)\n\u001b[32m    643\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m past_seen_tokens + sequence_length + \u001b[32m1\u001b[39m\n\u001b[32m    644\u001b[39m     )\n\u001b[32m    646\u001b[39m \u001b[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m causal_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_4d_causal_attention_mask_with_cache_position\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    658\u001b[39m     \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[32m    666\u001b[39m     min_dtype = torch.finfo(dtype).min\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:709\u001b[39m, in \u001b[36mLlamaModel._prepare_4d_causal_attention_mask_with_cache_position\u001b[39m\u001b[34m(attention_mask, sequence_length, target_length, dtype, device, cache_position, batch_size, **kwargs)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    708\u001b[39m     min_dtype = torch.finfo(dtype).min\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m     causal_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sequence_length != \u001b[32m1\u001b[39m:\n\u001b[32m    713\u001b[39m         causal_mask = torch.triu(causal_mask, diagonal=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 23.69 GiB of which 15.91 GiB is free. Process 858019 has 934.00 MiB memory in use. Process 859749 has 934.00 MiB memory in use. Including non-PyTorch memory, this process has 5.92 GiB memory in use. Of the allocated memory 5.63 GiB is allocated by PyTorch, and 1003.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model(input_ids = one_batch['input_ids'].cuda(), attention_mask = one_batch['attention_mask'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | LlamaForCausalLM | 162 M  | train\n",
      "---------------------------------------------------\n",
      "162 M     Trainable params\n",
      "0         Non-trainable params\n",
      "162 M     Total params\n",
      "650.138   Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1053\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misolate_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/utilities/seed.py:44\u001b[39m, in \u001b[36misolate_rng\u001b[39m\u001b[34m(include_cuda)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A context manager that resets the global random state on exit to what it was before entering.\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[33;03mIt supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m states = \u001b[43m_collect_rng_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:137\u001b[39m, in \u001b[36m_collect_rng_states\u001b[39m\u001b[34m(include_cuda)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_cuda:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     states[\u001b[33m\"\u001b[39m\u001b[33mtorch.cuda\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_rng_state_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/cuda/random.py:47\u001b[39m, in \u001b[36mget_rng_state_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return a list of ByteTensor representing the random number states of all devices.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m results = \u001b[43m[\u001b[49m\u001b[43mget_rng_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/cuda/random.py:47\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return a list of ByteTensor representing the random number states of all devices.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m results = [\u001b[43mget_rng_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count())]\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/cuda/random.py:42\u001b[39m, in \u001b[36mget_rng_state\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m     41\u001b[39m default_generator = torch.cuda.default_generators[idx]\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m lit_llm = LitLLM(model=model)\n\u001b[32m     30\u001b[39m trainer = L.Trainer(max_steps=\u001b[32m50000\u001b[39m, accelerator=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlit_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:69\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_teardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[32m     71\u001b[39m     trainer.state.stage = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[39m, in \u001b[36mTrainer._teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_teardown\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1033\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[33;03m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mteardown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     loop = \u001b[38;5;28mself\u001b[39m._active_loop\n\u001b[32m   1037\u001b[39m     \u001b[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:536\u001b[39m, in \u001b[36mStrategy.teardown\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    535\u001b[39m     log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: moving model to CPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.precision_plugin.teardown()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/lightning/fabric/utilities/device_dtype_mixin.py:82\u001b[39m, in \u001b[36m_DeviceDtypeModuleMixin.cpu\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m _update_properties(\u001b[38;5;28mself\u001b[39m, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:1121\u001b[39m, in \u001b[36mModule.cpu\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) -> T:\n\u001b[32m   1113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[32m   1114\u001b[39m \n\u001b[32m   1115\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1119\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/torch/nn/modules/module.py:1121\u001b[39m, in \u001b[36mModule.cpu.<locals>.<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcpu\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) -> T:\n\u001b[32m   1113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the CPU.\u001b[39;00m\n\u001b[32m   1114\u001b[39m \n\u001b[32m   1115\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1119\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "class LitLLM(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    # def on_train_start(self):\n",
    "    #     initialize_weights(self.trainer, self.model, n_layer=self.model.config.n_layer, n_embd=self.model.config.n_embd)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        logits = self.model(**batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits[..., :-1, :], batch[\"input_ids\"][..., 1:])\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        warmup_steps = 500\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=4e-4, weight_decay=0.1, betas=(0.9, 0.95))\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step / warmup_steps, 1.0))\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train_dataset = ld.StreamingDataset(\n",
    "#         input_dir=\"s3://tinyllama-template/slimpajama/train\",\n",
    "#         item_loader=ld.TokensLoader(block_size=128),\n",
    "#     )\n",
    "#     train_dataloader = ld.StreamingDataLoader(train_dataset, shuffle=True, batch_size=12, num_workers=4)\n",
    "\n",
    "lit_llm = LitLLM(model=model)\n",
    "trainer = L.Trainer(max_steps=50000, accelerator='cuda')\n",
    "trainer.fit(lit_llm, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0215, -0.0155, -0.0413,  ..., -0.0011,  0.0492, -0.0121],\n",
       "         [-0.0091, -0.0120,  0.0028,  ..., -0.0049, -0.0189, -0.0090],\n",
       "         [ 0.0268,  0.0064,  0.0055,  ...,  0.0113, -0.0038,  0.0080],\n",
       "         ...,\n",
       "         [ 0.0262,  0.0124,  0.0422,  ..., -0.0004,  0.0228, -0.0264],\n",
       "         [ 0.0004,  0.0083,  0.0308,  ..., -0.0025,  0.0214,  0.0233],\n",
       "         [-0.0161, -0.0297,  0.0026,  ...,  0.0260, -0.0261, -0.0002]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0132,  0.0042, -0.0003,  ..., -0.0299,  0.0231,  0.0126],\n",
       "         [-0.0053,  0.0099,  0.0160,  ..., -0.0094, -0.0092, -0.0075],\n",
       "         [-0.0013,  0.0065, -0.0130,  ..., -0.0031,  0.0073, -0.0014],\n",
       "         ...,\n",
       "         [ 0.0146, -0.0123,  0.0206,  ...,  0.0016,  0.0041,  0.0114],\n",
       "         [ 0.0030,  0.0406,  0.0192,  ...,  0.0028, -0.0166,  0.0336],\n",
       "         [-0.0128, -0.0135, -0.0073,  ..., -0.0029, -0.0171, -0.0381]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.2175e-03,  9.1038e-03,  1.5426e-02,  ...,  1.1674e-02,\n",
       "           1.7080e-02, -3.5558e-03],\n",
       "         [-2.1362e-02, -1.4282e-02, -3.4631e-02,  ..., -4.9443e-03,\n",
       "           7.6700e-03,  1.6871e-02],\n",
       "         [-2.8192e-02, -3.6324e-02,  2.0803e-02,  ..., -1.0396e-02,\n",
       "           1.9785e-02, -7.4489e-03],\n",
       "         ...,\n",
       "         [ 4.1918e-03, -1.4305e-02, -1.6614e-03,  ..., -9.0807e-06,\n",
       "          -2.6571e-02, -2.2606e-02],\n",
       "         [-3.3150e-03,  4.9351e-03, -1.1588e-02,  ...,  4.2913e-02,\n",
       "          -1.0119e-02,  2.5916e-02],\n",
       "         [-2.4322e-02, -1.5347e-02,  1.7416e-02,  ...,  1.2470e-02,\n",
       "          -1.1292e-02, -1.9129e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0001, -0.0210,  0.0104,  ...,  0.0044, -0.0063,  0.0337],\n",
       "         [ 0.0222,  0.0063, -0.0108,  ..., -0.0333, -0.0272,  0.0301],\n",
       "         [-0.0077,  0.0265, -0.0079,  ..., -0.0273,  0.0006, -0.0191],\n",
       "         ...,\n",
       "         [-0.0185,  0.0046,  0.0133,  ..., -0.0120,  0.0058, -0.0093],\n",
       "         [ 0.0234, -0.0341, -0.0040,  ..., -0.0029,  0.0069, -0.0046],\n",
       "         [-0.0264, -0.0122,  0.0022,  ..., -0.0177,  0.0207, -0.0054]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0273, -0.0145,  0.0017,  ..., -0.0092,  0.0376,  0.0164],\n",
       "         [-0.0118,  0.0104, -0.0189,  ...,  0.0007,  0.0030, -0.0049],\n",
       "         [-0.0172,  0.0092,  0.0150,  ..., -0.0015,  0.0152, -0.0017],\n",
       "         ...,\n",
       "         [-0.0341,  0.0344,  0.0399,  ...,  0.0108, -0.0045,  0.0123],\n",
       "         [ 0.0240,  0.0274,  0.0058,  ..., -0.0377,  0.0221, -0.0077],\n",
       "         [ 0.0082, -0.0098,  0.0137,  ..., -0.0055, -0.0199, -0.0254]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0252,  0.0167,  0.0150,  ...,  0.0067,  0.0167,  0.0187],\n",
       "         [-0.0225, -0.0156, -0.0003,  ..., -0.0305, -0.0302,  0.0164],\n",
       "         [-0.0108, -0.0104,  0.0006,  ..., -0.0072, -0.0236,  0.0501],\n",
       "         ...,\n",
       "         [ 0.0234, -0.0041,  0.0219,  ..., -0.0066,  0.0057, -0.0059],\n",
       "         [-0.0038, -0.0447,  0.0233,  ...,  0.0106,  0.0187,  0.0149],\n",
       "         [ 0.0265,  0.0076,  0.0023,  ..., -0.0261, -0.0237,  0.0191]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0006,  0.0160, -0.0020,  ...,  0.0101,  0.0549,  0.0061],\n",
       "         [-0.0205, -0.0221, -0.0028,  ...,  0.0037, -0.0094, -0.0320],\n",
       "         [ 0.0228,  0.0135, -0.0247,  ...,  0.0042, -0.0245, -0.0146],\n",
       "         ...,\n",
       "         [-0.0201,  0.0227, -0.0235,  ..., -0.0277, -0.0012,  0.0212],\n",
       "         [ 0.0244, -0.0159, -0.0194,  ...,  0.0284, -0.0328, -0.0215],\n",
       "         [-0.0056,  0.0325,  0.0012,  ...,  0.0326, -0.0003,  0.0156]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0308,  0.0296,  0.0012,  ..., -0.0019, -0.0363,  0.0208],\n",
       "         [-0.0616, -0.0528,  0.0411,  ..., -0.0309, -0.0285, -0.0211],\n",
       "         [-0.0088, -0.0079, -0.0094,  ..., -0.0219, -0.0165, -0.0002],\n",
       "         ...,\n",
       "         [-0.0084,  0.0036,  0.0387,  ...,  0.0267,  0.0131, -0.0241],\n",
       "         [ 0.0081, -0.0126,  0.0225,  ..., -0.0222, -0.0187,  0.0369],\n",
       "         [ 0.0097, -0.0288, -0.0179,  ...,  0.0113, -0.0096,  0.0114]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0251, -0.0344, -0.0104,  ...,  0.0006,  0.0065,  0.0172],\n",
       "         [ 0.0049, -0.0200, -0.0258,  ..., -0.0190, -0.0079,  0.0004],\n",
       "         [-0.0169, -0.0446,  0.0130,  ..., -0.0146,  0.0410,  0.0272],\n",
       "         ...,\n",
       "         [ 0.0360,  0.0203, -0.0013,  ...,  0.0021,  0.0105, -0.0403],\n",
       "         [ 0.0177, -0.0297, -0.0235,  ..., -0.0214, -0.0015,  0.0403],\n",
       "         [ 0.0206, -0.0239, -0.0135,  ..., -0.0253,  0.0116,  0.0208]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0094, -0.0102, -0.0185,  ...,  0.0075,  0.0039,  0.0156],\n",
       "         [ 0.0177,  0.0421,  0.0282,  ...,  0.0316,  0.0198,  0.0015],\n",
       "         [ 0.0166,  0.0028,  0.0343,  ..., -0.0101, -0.0045,  0.0062],\n",
       "         ...,\n",
       "         [-0.0111,  0.0071, -0.0006,  ...,  0.0346,  0.0361,  0.0361],\n",
       "         [-0.0064, -0.0154,  0.0019,  ..., -0.0045,  0.0248, -0.0107],\n",
       "         [ 0.0056, -0.0084,  0.0320,  ...,  0.0296, -0.0222, -0.0129]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0021,  0.0207,  0.0290,  ...,  0.0128, -0.0142, -0.0136],\n",
       "         [-0.0143, -0.0106, -0.0314,  ..., -0.0113,  0.0176, -0.0163],\n",
       "         [ 0.0100, -0.0055,  0.0036,  ..., -0.0183,  0.0185, -0.0017],\n",
       "         ...,\n",
       "         [ 0.0129,  0.0087, -0.0247,  ...,  0.0049, -0.0069, -0.0140],\n",
       "         [-0.0299,  0.0247,  0.0053,  ..., -0.0267, -0.0063, -0.0187],\n",
       "         [ 0.0190, -0.0110,  0.0027,  ..., -0.0264,  0.0043,  0.0229]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0077,  0.0067, -0.0131,  ..., -0.0225, -0.0208,  0.0292],\n",
       "         [ 0.0502,  0.0152,  0.0170,  ..., -0.0265, -0.0128,  0.0395],\n",
       "         [-0.0065,  0.0019,  0.0033,  ...,  0.0085, -0.0264,  0.0078],\n",
       "         ...,\n",
       "         [ 0.0421, -0.0374,  0.0411,  ...,  0.0169,  0.0269,  0.0179],\n",
       "         [ 0.0222,  0.0126, -0.0218,  ..., -0.0315, -0.0188, -0.0003],\n",
       "         [ 0.0094,  0.0238, -0.0221,  ..., -0.0041,  0.0181, -0.0030]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0137, -0.0119,  0.0105,  ..., -0.0208,  0.0388,  0.0044],\n",
       "         [-0.0009,  0.0251, -0.0210,  ...,  0.0106,  0.0322, -0.0005],\n",
       "         [ 0.0293,  0.0050, -0.0172,  ...,  0.0285, -0.0013,  0.0088],\n",
       "         ...,\n",
       "         [ 0.0222,  0.0249, -0.0120,  ..., -0.0111,  0.0388, -0.0199],\n",
       "         [ 0.0286, -0.0087,  0.0146,  ...,  0.0026,  0.0345,  0.0215],\n",
       "         [-0.0040,  0.0166,  0.0312,  ...,  0.0316,  0.0379, -0.0175]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0344, -0.0214,  0.0240,  ..., -0.0056, -0.0166,  0.0269],\n",
       "         [-0.0228, -0.0040,  0.0220,  ...,  0.0090, -0.0318,  0.0459],\n",
       "         [ 0.0028, -0.0295, -0.0143,  ...,  0.0114, -0.0042,  0.0259],\n",
       "         ...,\n",
       "         [ 0.0322, -0.0182,  0.0197,  ...,  0.0039, -0.0103,  0.0060],\n",
       "         [ 0.0070,  0.0144,  0.0149,  ...,  0.0206,  0.0364,  0.0169],\n",
       "         [-0.0158,  0.0163, -0.0131,  ...,  0.0077, -0.0034,  0.0016]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0038,  0.0156, -0.0017,  ..., -0.0154,  0.0106, -0.0094],\n",
       "         [ 0.0030,  0.0023,  0.0087,  ...,  0.0444, -0.0116, -0.0009],\n",
       "         [ 0.0443,  0.0248,  0.0010,  ..., -0.0270,  0.0245, -0.0320],\n",
       "         ...,\n",
       "         [-0.0231, -0.0133,  0.0073,  ...,  0.0041,  0.0294,  0.0045],\n",
       "         [ 0.0163,  0.0215,  0.0012,  ...,  0.0072,  0.0382, -0.0001],\n",
       "         [-0.0594, -0.0201, -0.0104,  ...,  0.0161, -0.0081, -0.0002]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0064,  0.0032,  0.0406,  ...,  0.0082, -0.0233,  0.0334],\n",
       "         [ 0.0399,  0.0230, -0.0124,  ..., -0.0120, -0.0307, -0.0010],\n",
       "         [ 0.0040, -0.0062,  0.0036,  ..., -0.0090,  0.0238, -0.0065],\n",
       "         ...,\n",
       "         [-0.0012,  0.0040, -0.0107,  ..., -0.0300, -0.0066,  0.0189],\n",
       "         [-0.0293, -0.0071,  0.0163,  ...,  0.0004, -0.0030,  0.0121],\n",
       "         [-0.0257, -0.0340,  0.0401,  ...,  0.0151, -0.0032,  0.0133]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0150,  0.0082,  0.0118,  ...,  0.0107, -0.0103,  0.0041],\n",
       "         [-0.0270,  0.0241,  0.0008,  ..., -0.0170,  0.0470,  0.0184],\n",
       "         [-0.0170,  0.0104, -0.0131,  ..., -0.0264,  0.0442, -0.0311],\n",
       "         ...,\n",
       "         [-0.0232, -0.0064, -0.0025,  ...,  0.0150,  0.0042, -0.0136],\n",
       "         [-0.0014,  0.0240,  0.0099,  ..., -0.0057,  0.0288, -0.0022],\n",
       "         [-0.0173, -0.0061, -0.0059,  ...,  0.0076,  0.0046, -0.0262]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0127,  0.0326, -0.0280,  ..., -0.0109,  0.0127,  0.0148],\n",
       "         [ 0.0104,  0.0293,  0.0355,  ..., -0.0168,  0.0009, -0.0027],\n",
       "         [-0.0056,  0.0274, -0.0167,  ...,  0.0090, -0.0153,  0.0045],\n",
       "         ...,\n",
       "         [ 0.0145, -0.0118,  0.0057,  ..., -0.0125, -0.0075,  0.0289],\n",
       "         [ 0.0009, -0.0095, -0.0038,  ...,  0.0107, -0.0019, -0.0378],\n",
       "         [ 0.0091,  0.0140, -0.0131,  ...,  0.0065, -0.0141,  0.0161]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0064, -0.0026, -0.0346,  ..., -0.0082,  0.0223,  0.0061],\n",
       "         [ 0.0053, -0.0344,  0.0193,  ...,  0.0024, -0.0121,  0.0494],\n",
       "         [-0.0111,  0.0264,  0.0235,  ..., -0.0289, -0.0332, -0.0431],\n",
       "         ...,\n",
       "         [-0.0091,  0.0097, -0.0190,  ..., -0.0223,  0.0020, -0.0111],\n",
       "         [-0.0291,  0.0165,  0.0531,  ...,  0.0008, -0.0060, -0.0134],\n",
       "         [ 0.0297,  0.0151,  0.0331,  ..., -0.0069,  0.0143,  0.0061]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0006,  0.0102, -0.0218,  ..., -0.0258,  0.0308,  0.0074],\n",
       "         [ 0.0103, -0.0053,  0.0153,  ..., -0.0233,  0.0167, -0.0096],\n",
       "         [-0.0041, -0.0010, -0.0210,  ..., -0.0021, -0.0009, -0.0020],\n",
       "         ...,\n",
       "         [-0.0112, -0.0179,  0.0075,  ..., -0.0066, -0.0065,  0.0048],\n",
       "         [ 0.0186, -0.0146,  0.0247,  ...,  0.0163,  0.0142, -0.0024],\n",
       "         [-0.0207, -0.0197,  0.0365,  ...,  0.0250,  0.0237, -0.0285]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0423,  0.0115,  0.0018,  ...,  0.0230,  0.0140, -0.0329],\n",
       "         [-0.0196,  0.0006,  0.0096,  ...,  0.0073, -0.0114, -0.0050],\n",
       "         [-0.0257, -0.0032, -0.0183,  ...,  0.0126,  0.0194,  0.0025],\n",
       "         ...,\n",
       "         [-0.0604,  0.0046, -0.0225,  ...,  0.0143,  0.0170, -0.0115],\n",
       "         [-0.0015, -0.0461, -0.0041,  ...,  0.0371,  0.0441,  0.0150],\n",
       "         [ 0.0105,  0.0034, -0.0144,  ...,  0.0285, -0.0027, -0.0120]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-1.4703e-02, -2.5086e-02,  2.7792e-02,  ...,  3.7231e-03,\n",
       "          -4.1127e-02, -4.3490e-03],\n",
       "         [ 2.6328e-02,  2.9056e-03, -3.9420e-02,  ...,  7.6306e-03,\n",
       "          -5.8441e-02, -9.6271e-05],\n",
       "         [-1.2793e-02,  2.4823e-03,  1.6837e-02,  ..., -1.0296e-02,\n",
       "           1.7691e-02,  2.3900e-02],\n",
       "         ...,\n",
       "         [ 2.5911e-02, -1.7370e-02, -3.0912e-02,  ...,  6.7620e-03,\n",
       "           1.2069e-03, -1.1640e-02],\n",
       "         [-2.3884e-02,  7.1007e-03,  2.6490e-02,  ..., -1.7374e-02,\n",
       "           1.5420e-02, -3.2626e-03],\n",
       "         [ 3.1435e-03, -1.4639e-02, -6.3296e-03,  ...,  1.5527e-02,\n",
       "           3.0604e-02,  8.1659e-03]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0095,  0.0207,  0.0289,  ...,  0.0279, -0.0031, -0.0129],\n",
       "         [-0.0263, -0.0249,  0.0196,  ...,  0.0175,  0.0104,  0.0159],\n",
       "         [-0.0008,  0.0010, -0.0124,  ...,  0.0148,  0.0182, -0.0123],\n",
       "         ...,\n",
       "         [-0.0167, -0.0163,  0.0091,  ..., -0.0038,  0.0143, -0.0147],\n",
       "         [ 0.0208,  0.0121, -0.0297,  ..., -0.0040,  0.0096, -0.0049],\n",
       "         [ 0.0240,  0.0173,  0.0146,  ...,  0.0026, -0.0172, -0.0451]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0293, -0.0283,  0.0213,  ..., -0.0091, -0.0202, -0.0165],\n",
       "         [-0.0163,  0.0138,  0.0355,  ..., -0.0158, -0.0169,  0.0077],\n",
       "         [-0.0300, -0.0072, -0.0411,  ...,  0.0010,  0.0050,  0.0230],\n",
       "         ...,\n",
       "         [ 0.0083, -0.0247, -0.0070,  ...,  0.0249,  0.0174, -0.0126],\n",
       "         [ 0.0022, -0.0106, -0.0324,  ..., -0.0499, -0.0120,  0.0089],\n",
       "         [ 0.0055,  0.0053,  0.0091,  ..., -0.0037,  0.0237,  0.0091]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0407, -0.0065, -0.0053,  ..., -0.0046, -0.0098,  0.0181],\n",
       "         [ 0.0201,  0.0063, -0.0012,  ..., -0.0437, -0.0080, -0.0071],\n",
       "         [ 0.0139,  0.0121,  0.0037,  ..., -0.0062,  0.0188,  0.0014],\n",
       "         ...,\n",
       "         [-0.0082, -0.0083,  0.0227,  ...,  0.0050,  0.0324, -0.0293],\n",
       "         [-0.0197, -0.0020, -0.0058,  ..., -0.0252,  0.0042, -0.0389],\n",
       "         [ 0.0162,  0.0025,  0.0004,  ...,  0.0048,  0.0102,  0.0313]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0102, -0.0075, -0.0271,  ...,  0.0326,  0.0010, -0.0035],\n",
       "         [ 0.0224,  0.0245,  0.0205,  ...,  0.0451, -0.0016, -0.0033],\n",
       "         [-0.0036,  0.0150, -0.0183,  ...,  0.0413, -0.0453,  0.0003],\n",
       "         ...,\n",
       "         [ 0.0087,  0.0437, -0.0414,  ..., -0.0060,  0.0143,  0.0297],\n",
       "         [-0.0046,  0.0081, -0.0022,  ..., -0.0140,  0.0167, -0.0136],\n",
       "         [-0.0070, -0.0190,  0.0070,  ...,  0.0118, -0.0250, -0.0318]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0067, -0.0223,  0.0487,  ..., -0.0453,  0.0094,  0.0151],\n",
       "         [-0.0118, -0.0106, -0.0161,  ...,  0.0027, -0.0131,  0.0238],\n",
       "         [ 0.0037, -0.0067, -0.0053,  ...,  0.0019, -0.0311, -0.0034],\n",
       "         ...,\n",
       "         [-0.0077, -0.0080, -0.0173,  ..., -0.0157, -0.0145,  0.0093],\n",
       "         [-0.0073,  0.0053,  0.0025,  ...,  0.0373, -0.0110,  0.0117],\n",
       "         [-0.0124, -0.0048, -0.0275,  ..., -0.0159, -0.0068, -0.0040]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0066, -0.0333, -0.0002,  ..., -0.0215, -0.0022, -0.0162],\n",
       "         [ 0.0334, -0.0095,  0.0218,  ...,  0.0556,  0.0002,  0.0200],\n",
       "         [ 0.0022, -0.0033, -0.0181,  ..., -0.0478, -0.0270, -0.0034],\n",
       "         ...,\n",
       "         [-0.0115, -0.0296,  0.0060,  ...,  0.0020, -0.0011, -0.0200],\n",
       "         [ 0.0006,  0.0335,  0.0366,  ..., -0.0164, -0.0304,  0.0345],\n",
       "         [-0.0027,  0.0055, -0.0100,  ..., -0.0093,  0.0066, -0.0144]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0096, -0.0040, -0.0348,  ...,  0.0281,  0.0296, -0.0127],\n",
       "         [ 0.0144,  0.0069, -0.0120,  ..., -0.0263, -0.0361, -0.0031],\n",
       "         [ 0.0031, -0.0098,  0.0024,  ...,  0.0156,  0.0014, -0.0465],\n",
       "         ...,\n",
       "         [ 0.0008, -0.0056, -0.0327,  ...,  0.0161,  0.0314,  0.0102],\n",
       "         [-0.0255, -0.0394, -0.0085,  ..., -0.0004, -0.0310,  0.0002],\n",
       "         [ 0.0120,  0.0165,  0.0011,  ...,  0.0253,  0.0147,  0.0232]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0009, -0.0127,  0.0141,  ..., -0.0093, -0.0182,  0.0049],\n",
       "         [-0.0310, -0.0272, -0.0267,  ..., -0.0234, -0.0014,  0.0068],\n",
       "         [ 0.0069, -0.0112,  0.0069,  ...,  0.0014,  0.0065, -0.0277],\n",
       "         ...,\n",
       "         [ 0.0015,  0.0153, -0.0095,  ...,  0.0054,  0.0113, -0.0230],\n",
       "         [ 0.0236, -0.0045,  0.0250,  ..., -0.0138,  0.0016, -0.0092],\n",
       "         [ 0.0192,  0.0045,  0.0273,  ..., -0.0216,  0.0027, -0.0066]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.4891e-02,  1.4779e-03,  3.7258e-02,  ..., -1.1287e-02,\n",
       "           4.7843e-03,  1.3658e-02],\n",
       "         [ 1.1134e-03,  2.5230e-02,  5.2227e-05,  ...,  1.9704e-02,\n",
       "           1.0276e-02, -2.1528e-02],\n",
       "         [-1.2205e-02,  1.7524e-02, -2.1606e-02,  ..., -6.2747e-03,\n",
       "           2.3303e-02,  1.3451e-02],\n",
       "         ...,\n",
       "         [ 2.3391e-02,  2.8273e-02, -2.3029e-02,  ...,  1.7872e-03,\n",
       "           1.0765e-02,  3.8925e-03],\n",
       "         [ 1.2169e-02, -2.8574e-02,  1.9074e-03,  ...,  1.6664e-02,\n",
       "          -1.3920e-02, -2.0715e-03],\n",
       "         [ 3.8912e-03,  9.1978e-03,  2.1082e-03,  ..., -1.4310e-02,\n",
       "           2.5298e-02,  3.8241e-04]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0016,  0.0200,  0.0030,  ...,  0.0142,  0.0137,  0.0096],\n",
       "         [ 0.0176,  0.0035, -0.0148,  ..., -0.0090, -0.0124, -0.0300],\n",
       "         [-0.0038, -0.0196,  0.0376,  ..., -0.0254,  0.0442,  0.0024],\n",
       "         ...,\n",
       "         [ 0.0138, -0.0051,  0.0072,  ..., -0.0356,  0.0082,  0.0290],\n",
       "         [ 0.0195,  0.0164,  0.0210,  ..., -0.0055,  0.0043,  0.0112],\n",
       "         [-0.0091,  0.0137, -0.0117,  ...,  0.0215,  0.0410, -0.0006]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0139,  0.0178,  0.0249,  ...,  0.0050, -0.0284, -0.0036],\n",
       "         [ 0.0165, -0.0112, -0.0049,  ..., -0.0481, -0.0424,  0.0079],\n",
       "         [-0.0114, -0.0303, -0.0278,  ...,  0.0334,  0.0098,  0.0157],\n",
       "         ...,\n",
       "         [ 0.0107, -0.0024,  0.0192,  ..., -0.0137,  0.0089,  0.0213],\n",
       "         [ 0.0066, -0.0283,  0.0098,  ...,  0.0252,  0.0196,  0.0094],\n",
       "         [ 0.0521, -0.0008,  0.0188,  ..., -0.0391, -0.0060, -0.0133]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0155, -0.0116,  0.0090,  ...,  0.0249,  0.0027, -0.0092],\n",
       "         [-0.0163,  0.0329, -0.0264,  ...,  0.0224,  0.0207, -0.0198],\n",
       "         [ 0.0015, -0.0002, -0.0115,  ...,  0.0229,  0.0186, -0.0191],\n",
       "         ...,\n",
       "         [-0.0143,  0.0005, -0.0275,  ...,  0.0278, -0.0119, -0.0040],\n",
       "         [-0.0021, -0.0236, -0.0266,  ...,  0.0082, -0.0031, -0.0048],\n",
       "         [-0.0084,  0.0180,  0.0026,  ..., -0.0061,  0.0381, -0.0517]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0352,  0.0341,  0.0295,  ...,  0.0026, -0.0297, -0.0221],\n",
       "         [-0.0022,  0.0277,  0.0004,  ..., -0.0051, -0.0279,  0.0443],\n",
       "         [-0.0019, -0.0245,  0.0328,  ...,  0.0234,  0.0127,  0.0098],\n",
       "         ...,\n",
       "         [-0.0191, -0.0206,  0.0437,  ...,  0.0252,  0.0680,  0.0096],\n",
       "         [ 0.0192, -0.0213,  0.0081,  ..., -0.0480, -0.0201, -0.0186],\n",
       "         [-0.0107, -0.0024, -0.0135,  ...,  0.0289,  0.0220,  0.0148]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0238, -0.0063,  0.0231,  ..., -0.0301,  0.0145,  0.0058],\n",
       "         [ 0.0141, -0.0106,  0.0170,  ..., -0.0165,  0.0271, -0.0033],\n",
       "         [ 0.0301, -0.0044, -0.0064,  ..., -0.0179,  0.0077, -0.0353],\n",
       "         ...,\n",
       "         [-0.0220, -0.0002, -0.0101,  ...,  0.0273,  0.0073, -0.0253],\n",
       "         [ 0.0194, -0.0107,  0.0146,  ..., -0.0169,  0.0137,  0.0010],\n",
       "         [-0.0395,  0.0021, -0.0072,  ...,  0.0059, -0.0081, -0.0282]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0186, -0.0105,  0.0504,  ..., -0.0116,  0.0584, -0.0113],\n",
       "         [ 0.0117,  0.0420, -0.0076,  ...,  0.0265, -0.0297,  0.0083],\n",
       "         [ 0.0093, -0.0415, -0.0071,  ...,  0.0023, -0.0059,  0.0086],\n",
       "         ...,\n",
       "         [-0.0054,  0.0050,  0.0229,  ...,  0.0311,  0.0073, -0.0110],\n",
       "         [ 0.0010, -0.0017, -0.0066,  ...,  0.0392, -0.0040, -0.0020],\n",
       "         [ 0.0217, -0.0246,  0.0396,  ...,  0.0178, -0.0093,  0.0032]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0252, -0.0230, -0.0102,  ..., -0.0097, -0.0436, -0.0085],\n",
       "         [ 0.0040,  0.0149,  0.0170,  ...,  0.0084,  0.0546,  0.0005],\n",
       "         [ 0.0305,  0.0046, -0.0204,  ...,  0.0181,  0.0203, -0.0016],\n",
       "         ...,\n",
       "         [-0.0106, -0.0280, -0.0190,  ...,  0.0169,  0.0264, -0.0160],\n",
       "         [ 0.0094, -0.0078, -0.0417,  ..., -0.0193, -0.0089, -0.0209],\n",
       "         [ 0.0175,  0.0291,  0.0181,  ..., -0.0346,  0.0121, -0.0118]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0258,  0.0178,  0.0195,  ..., -0.0092,  0.0202,  0.0215],\n",
       "         [-0.0303, -0.0062,  0.0128,  ..., -0.0094,  0.0017,  0.0330],\n",
       "         [-0.0077, -0.0050, -0.0069,  ..., -0.0076,  0.0415,  0.0035],\n",
       "         ...,\n",
       "         [ 0.0040,  0.0074,  0.0011,  ..., -0.0197,  0.0157, -0.0084],\n",
       "         [ 0.0190, -0.0113,  0.0182,  ..., -0.0482,  0.0201, -0.0009],\n",
       "         [ 0.0156, -0.0493,  0.0135,  ..., -0.0247,  0.0105,  0.0028]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0039, -0.0014,  0.0347,  ...,  0.0132, -0.0120, -0.0232],\n",
       "         [-0.0336, -0.0145, -0.0064,  ...,  0.0119, -0.0176,  0.0189],\n",
       "         [-0.0267,  0.0161, -0.0162,  ..., -0.0285,  0.0149, -0.0022],\n",
       "         ...,\n",
       "         [-0.0287, -0.0237, -0.0137,  ...,  0.0040,  0.0110,  0.0152],\n",
       "         [ 0.0104,  0.0298, -0.0156,  ..., -0.0199,  0.0178,  0.0176],\n",
       "         [ 0.0057, -0.0045,  0.0127,  ...,  0.0185,  0.0318,  0.0118]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0126,  0.0074, -0.0109,  ...,  0.0201, -0.0195,  0.0249],\n",
       "         [-0.0094, -0.0203, -0.0092,  ...,  0.0191,  0.0022, -0.0033],\n",
       "         [ 0.0085, -0.0150,  0.0020,  ...,  0.0173,  0.0080,  0.0168],\n",
       "         ...,\n",
       "         [ 0.0117,  0.0027,  0.0138,  ..., -0.0106, -0.0321, -0.0028],\n",
       "         [ 0.0156,  0.0010,  0.0184,  ...,  0.0303, -0.0313, -0.0299],\n",
       "         [-0.0050, -0.0214,  0.0128,  ...,  0.0045, -0.0039, -0.0264]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0138,  0.0183, -0.0176,  ...,  0.0109, -0.0084,  0.0053],\n",
       "         [-0.0113,  0.0050,  0.0126,  ...,  0.0256, -0.0269, -0.0172],\n",
       "         [-0.0171, -0.0124, -0.0053,  ...,  0.0407,  0.0015, -0.0295],\n",
       "         ...,\n",
       "         [ 0.0297,  0.0020, -0.0345,  ...,  0.0046,  0.0038,  0.0117],\n",
       "         [-0.0023, -0.0039, -0.0278,  ..., -0.0032, -0.0143,  0.0057],\n",
       "         [ 0.0416, -0.0159,  0.0047,  ..., -0.0015, -0.0062,  0.0057]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0131, -0.0009,  0.0090,  ..., -0.0189,  0.0159, -0.0263],\n",
       "         [ 0.0106,  0.0044,  0.0167,  ..., -0.0023, -0.0021,  0.0196],\n",
       "         [-0.0051, -0.0310, -0.0031,  ..., -0.0027, -0.0335,  0.0211],\n",
       "         ...,\n",
       "         [ 0.0411,  0.0136, -0.0120,  ...,  0.0160, -0.0016,  0.0023],\n",
       "         [-0.0021,  0.0176,  0.0091,  ..., -0.0166,  0.0177,  0.0165],\n",
       "         [ 0.0217,  0.0191, -0.0069,  ..., -0.0008, -0.0162,  0.0112]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0225,  0.0007,  0.0095,  ..., -0.0208,  0.0261, -0.0168],\n",
       "         [ 0.0040, -0.0099,  0.0005,  ..., -0.0152,  0.0019, -0.0372],\n",
       "         [ 0.0182, -0.0128,  0.0223,  ...,  0.0250, -0.0187,  0.0119],\n",
       "         ...,\n",
       "         [ 0.0074, -0.0376,  0.0007,  ..., -0.0311, -0.0171, -0.0291],\n",
       "         [ 0.0144, -0.0227, -0.0218,  ...,  0.0198, -0.0158, -0.0004],\n",
       "         [ 0.0066,  0.0169, -0.0028,  ...,  0.0046, -0.0313,  0.0027]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0219,  0.0224, -0.0039,  ...,  0.0474,  0.0014, -0.0134],\n",
       "         [-0.0070,  0.0169, -0.0058,  ...,  0.0371,  0.0134,  0.0033],\n",
       "         [ 0.0114,  0.0030,  0.0154,  ..., -0.0104, -0.0469,  0.0400],\n",
       "         ...,\n",
       "         [-0.0047,  0.0051,  0.0193,  ...,  0.0016,  0.0274, -0.0130],\n",
       "         [-0.0124,  0.0215,  0.0173,  ...,  0.0197, -0.0017, -0.0157],\n",
       "         [-0.0305, -0.0027, -0.0134,  ...,  0.0301, -0.0129,  0.0052]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0034, -0.0401,  0.0583,  ..., -0.0047,  0.0074, -0.0229],\n",
       "         [ 0.0104,  0.0061,  0.0094,  ..., -0.0107,  0.0344,  0.0042],\n",
       "         [-0.0152, -0.0053, -0.0188,  ..., -0.0136, -0.0133, -0.0144],\n",
       "         ...,\n",
       "         [ 0.0092,  0.0562, -0.0252,  ...,  0.0119, -0.0146, -0.0109],\n",
       "         [ 0.0214,  0.0042,  0.0263,  ..., -0.0082, -0.0117,  0.0088],\n",
       "         [-0.0027, -0.0202, -0.0133,  ..., -0.0199, -0.0053, -0.0056]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0219,  0.0285, -0.0357,  ...,  0.0304, -0.0010, -0.0326],\n",
       "         [-0.0248,  0.0185,  0.0081,  ...,  0.0454,  0.0079,  0.0065],\n",
       "         [-0.0406,  0.0036,  0.0072,  ..., -0.0025,  0.0092, -0.0269],\n",
       "         ...,\n",
       "         [-0.0228, -0.0230, -0.0143,  ...,  0.0143,  0.0186, -0.0281],\n",
       "         [ 0.0019, -0.0164, -0.0016,  ...,  0.0233,  0.0404, -0.0114],\n",
       "         [-0.0100, -0.0367, -0.0018,  ..., -0.0036, -0.0011,  0.0108]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0169, -0.0479, -0.0111,  ...,  0.0188, -0.0128, -0.0072],\n",
       "         [-0.0599, -0.0225,  0.0044,  ..., -0.0184, -0.0008,  0.0281],\n",
       "         [ 0.0039, -0.0210, -0.0120,  ...,  0.0105, -0.0074,  0.0040],\n",
       "         ...,\n",
       "         [-0.0258,  0.0246,  0.0088,  ...,  0.0103, -0.0067,  0.0134],\n",
       "         [ 0.0170, -0.0151,  0.0093,  ..., -0.0129,  0.0005,  0.0239],\n",
       "         [-0.0008, -0.0329,  0.0150,  ..., -0.0062, -0.0155,  0.0112]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0118, -0.0067, -0.0175,  ...,  0.0013,  0.0265,  0.0168],\n",
       "         [-0.0121, -0.0176, -0.0453,  ...,  0.0032, -0.0022,  0.0280],\n",
       "         [-0.0072,  0.0095,  0.0378,  ...,  0.0107,  0.0022,  0.0225],\n",
       "         ...,\n",
       "         [-0.0145, -0.0200,  0.0398,  ...,  0.0186, -0.0049, -0.0128],\n",
       "         [-0.0015, -0.0332,  0.0409,  ...,  0.0115,  0.0115, -0.0078],\n",
       "         [ 0.0085, -0.0338, -0.0240,  ...,  0.0298,  0.0522,  0.0030]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0153,  0.0144, -0.0110,  ..., -0.0143, -0.0049,  0.0002],\n",
       "         [-0.0031, -0.0108, -0.0271,  ...,  0.0255, -0.0081,  0.0173],\n",
       "         [ 0.0286,  0.0062, -0.0179,  ...,  0.0302,  0.0132,  0.0219],\n",
       "         ...,\n",
       "         [-0.0184,  0.0309,  0.0003,  ..., -0.0387,  0.0430, -0.0036],\n",
       "         [-0.0011, -0.0067, -0.0025,  ..., -0.0045, -0.0333, -0.0002],\n",
       "         [ 0.0126,  0.0034, -0.0074,  ...,  0.0121, -0.0026,  0.0055]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0015,  0.0248, -0.0350,  ..., -0.0062,  0.0052, -0.0113],\n",
       "         [ 0.0363, -0.0030, -0.0078,  ..., -0.0027, -0.0220, -0.0184],\n",
       "         [ 0.0075, -0.0053,  0.0246,  ...,  0.0460,  0.0099, -0.0011],\n",
       "         ...,\n",
       "         [ 0.0132,  0.0047,  0.0170,  ..., -0.0137,  0.0141,  0.0353],\n",
       "         [ 0.0167,  0.0060, -0.0138,  ..., -0.0079,  0.0051, -0.0158],\n",
       "         [-0.0374,  0.0436, -0.0019,  ..., -0.0143,  0.0259, -0.0100]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0379, -0.0132, -0.0086,  ..., -0.0074, -0.0138, -0.0175],\n",
       "         [-0.0267, -0.0199,  0.0237,  ...,  0.0125,  0.0070,  0.0051],\n",
       "         [-0.0236, -0.0053, -0.0373,  ...,  0.0011,  0.0145,  0.0201],\n",
       "         ...,\n",
       "         [-0.0040,  0.0011,  0.0030,  ..., -0.0370, -0.0123, -0.0049],\n",
       "         [-0.0110, -0.0099,  0.0195,  ...,  0.0200,  0.0060, -0.0015],\n",
       "         [ 0.0036,  0.0143,  0.0127,  ..., -0.0006, -0.0161, -0.0109]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0112, -0.0058, -0.0148,  ...,  0.0123, -0.0064,  0.0157],\n",
       "         [-0.0014,  0.0164, -0.0166,  ..., -0.0190, -0.0371, -0.0420],\n",
       "         [-0.0065,  0.0071, -0.0025,  ...,  0.0104, -0.0091,  0.0037],\n",
       "         ...,\n",
       "         [ 0.0203,  0.0071,  0.0223,  ...,  0.0015,  0.0047,  0.0237],\n",
       "         [ 0.0331, -0.0090, -0.0237,  ..., -0.0192, -0.0016,  0.0613],\n",
       "         [ 0.0429,  0.0111,  0.0556,  ...,  0.0068,  0.0037,  0.0290]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0142, -0.0250, -0.0149,  ...,  0.0050, -0.0394,  0.0104],\n",
       "         [-0.0329, -0.0023, -0.0371,  ..., -0.0034,  0.0227,  0.0101],\n",
       "         [-0.0195,  0.0045,  0.0033,  ...,  0.0004, -0.0034,  0.0359],\n",
       "         ...,\n",
       "         [ 0.0021,  0.0263, -0.0191,  ...,  0.0084,  0.0226, -0.0105],\n",
       "         [-0.0368,  0.0006, -0.0095,  ..., -0.0069,  0.0079,  0.0040],\n",
       "         [ 0.0022, -0.0369, -0.0100,  ...,  0.0038,  0.0356,  0.0071]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0065, -0.0088,  0.0186,  ...,  0.0234, -0.0175,  0.0020],\n",
       "         [-0.0102, -0.0306,  0.0278,  ...,  0.0059, -0.0064, -0.0098],\n",
       "         [-0.0070,  0.0095,  0.0219,  ...,  0.0055,  0.0096, -0.0186],\n",
       "         ...,\n",
       "         [ 0.0468, -0.0127,  0.0162,  ..., -0.0091, -0.0207, -0.0050],\n",
       "         [ 0.0356, -0.0249, -0.0217,  ..., -0.0093, -0.0023, -0.0200],\n",
       "         [-0.0004,  0.0059, -0.0204,  ..., -0.0166,  0.0263,  0.0119]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0308,  0.0047,  0.0019,  ..., -0.0027, -0.0148, -0.0002],\n",
       "         [ 0.0482, -0.0205, -0.0191,  ..., -0.0246,  0.0173, -0.0020],\n",
       "         [ 0.0090, -0.0085, -0.0289,  ...,  0.0063, -0.0054,  0.0062],\n",
       "         ...,\n",
       "         [-0.0193,  0.0083,  0.0105,  ..., -0.0309,  0.0049, -0.0126],\n",
       "         [ 0.0145,  0.0110, -0.0446,  ..., -0.0157,  0.0061,  0.0077],\n",
       "         [ 0.0122, -0.0127,  0.0131,  ..., -0.0040,  0.0041, -0.0086]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0094,  0.0181, -0.0080,  ...,  0.0039,  0.0386,  0.0149],\n",
       "         [-0.0189,  0.0078, -0.0238,  ...,  0.0190, -0.0215,  0.0177],\n",
       "         [-0.0109, -0.0022, -0.0499,  ..., -0.0105,  0.0112,  0.0286],\n",
       "         ...,\n",
       "         [ 0.0083,  0.0523, -0.0146,  ...,  0.0360,  0.0032, -0.0195],\n",
       "         [-0.0006, -0.0089,  0.0115,  ...,  0.0286, -0.0144,  0.0022],\n",
       "         [-0.0138, -0.0132, -0.0271,  ..., -0.0381, -0.0236, -0.0021]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.9968e-02,  2.3332e-02, -3.1828e-02,  ..., -1.9057e-02,\n",
       "          -2.4796e-02, -5.9886e-03],\n",
       "         [-2.1264e-02,  7.1760e-03,  2.3788e-03,  ...,  3.0748e-02,\n",
       "           3.9373e-02, -5.6571e-03],\n",
       "         [ 4.2821e-02,  8.5000e-03, -6.8068e-03,  ..., -1.3477e-03,\n",
       "           1.7283e-04,  1.9869e-02],\n",
       "         ...,\n",
       "         [ 5.5948e-03,  1.8095e-02,  2.5329e-02,  ..., -3.2485e-02,\n",
       "          -3.5031e-02, -1.6302e-02],\n",
       "         [-1.5050e-02,  9.2258e-03, -5.1392e-04,  ...,  7.7933e-03,\n",
       "           1.1244e-02,  3.0990e-02],\n",
       "         [-3.6143e-02,  1.0242e-02, -1.0265e-02,  ...,  5.7750e-03,\n",
       "          -7.8177e-03,  7.5987e-06]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.1063e-02,  6.1321e-03,  5.0708e-03,  ...,  5.3658e-03,\n",
       "           6.8489e-03,  2.1961e-02],\n",
       "         [ 6.4077e-03, -2.3639e-04,  2.6499e-05,  ..., -1.0627e-02,\n",
       "          -1.0962e-02, -2.5140e-02],\n",
       "         [-1.9628e-02, -1.5964e-02,  2.6446e-02,  ...,  3.1303e-03,\n",
       "           3.5733e-02,  1.6084e-02],\n",
       "         ...,\n",
       "         [-2.6027e-02, -4.4729e-02,  7.4234e-04,  ..., -2.6221e-02,\n",
       "          -3.9190e-02,  2.5136e-03],\n",
       "         [-2.3753e-03, -6.6151e-03, -1.9810e-02,  ..., -4.1164e-02,\n",
       "          -3.6735e-02, -3.2389e-02],\n",
       "         [ 4.2282e-03,  1.3304e-02,  3.3953e-02,  ..., -1.4929e-02,\n",
       "          -9.9970e-03,  6.4604e-03]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0153, -0.0290,  0.0079,  ...,  0.0144, -0.0029,  0.0187],\n",
       "         [ 0.0237, -0.0150, -0.0022,  ...,  0.0258,  0.0046,  0.0143],\n",
       "         [ 0.0380,  0.0228,  0.0029,  ..., -0.0137, -0.0185, -0.0042],\n",
       "         ...,\n",
       "         [ 0.0243, -0.0075,  0.0262,  ...,  0.0009, -0.0192, -0.0224],\n",
       "         [ 0.0214,  0.0021,  0.0006,  ..., -0.0118, -0.0066,  0.0061],\n",
       "         [-0.0078, -0.0616,  0.0009,  ..., -0.0033,  0.0113, -0.0032]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0067,  0.0056,  0.0078,  ..., -0.0017,  0.0174, -0.0125],\n",
       "         [ 0.0025, -0.0198, -0.0359,  ...,  0.0117, -0.0344, -0.0303],\n",
       "         [ 0.0032,  0.0209,  0.0191,  ..., -0.0085, -0.0043,  0.0163],\n",
       "         ...,\n",
       "         [ 0.0207,  0.0073,  0.0013,  ...,  0.0073,  0.0241,  0.0129],\n",
       "         [ 0.0084,  0.0277, -0.0281,  ...,  0.0028, -0.0169,  0.0258],\n",
       "         [ 0.0331,  0.0034, -0.0082,  ..., -0.0386, -0.0100, -0.0172]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0030, -0.0099,  0.0090,  ...,  0.0009,  0.0113,  0.0283],\n",
       "         [-0.0023, -0.0268, -0.0426,  ..., -0.0038, -0.0040, -0.0092],\n",
       "         [-0.0018, -0.0244,  0.0229,  ..., -0.0193, -0.0047,  0.0145],\n",
       "         ...,\n",
       "         [ 0.0165,  0.0128,  0.0045,  ...,  0.0204,  0.0326, -0.0143],\n",
       "         [-0.0095, -0.0063,  0.0398,  ..., -0.0392, -0.0111, -0.0031],\n",
       "         [ 0.0100,  0.0192, -0.0033,  ..., -0.0307, -0.0342, -0.0141]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0107,  0.0218, -0.0201,  ...,  0.0013,  0.0018,  0.0460],\n",
       "         [-0.0160,  0.0121, -0.0053,  ..., -0.0337, -0.0256,  0.0193],\n",
       "         [-0.0129, -0.0201, -0.0168,  ..., -0.0213,  0.0391,  0.0050],\n",
       "         ...,\n",
       "         [-0.0160, -0.0424,  0.0216,  ...,  0.0371, -0.0009,  0.0248],\n",
       "         [ 0.0251, -0.0066,  0.0272,  ...,  0.0091, -0.0329,  0.0088],\n",
       "         [-0.0021,  0.0312,  0.0064,  ...,  0.0143, -0.0204, -0.0052]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0253,  0.0335, -0.0141,  ...,  0.0191, -0.0090, -0.0164],\n",
       "         [-0.0012, -0.0051,  0.0366,  ..., -0.0020, -0.0043, -0.0073],\n",
       "         [-0.0240, -0.0035, -0.0103,  ...,  0.0224,  0.0205, -0.0032],\n",
       "         ...,\n",
       "         [ 0.0143, -0.0272, -0.0138,  ..., -0.0071, -0.0020,  0.0025],\n",
       "         [-0.0231, -0.0363,  0.0068,  ...,  0.0155, -0.0057,  0.0049],\n",
       "         [ 0.0055,  0.0153, -0.0026,  ...,  0.0278,  0.0060,  0.0102]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0194,  0.0281,  0.0001,  ..., -0.0299, -0.0029,  0.0330],\n",
       "         [-0.0127, -0.0057, -0.0055,  ..., -0.0120,  0.0368, -0.0085],\n",
       "         [-0.0094,  0.0189,  0.0290,  ..., -0.0145,  0.0036, -0.0139],\n",
       "         ...,\n",
       "         [-0.0356,  0.0086,  0.0100,  ...,  0.0035, -0.0252,  0.0172],\n",
       "         [-0.0136, -0.0152,  0.0320,  ...,  0.0277,  0.0038,  0.0193],\n",
       "         [-0.0010, -0.0617, -0.0080,  ..., -0.0423,  0.0429, -0.0003]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0188, -0.0297, -0.0156,  ..., -0.0301, -0.0152, -0.0162],\n",
       "         [-0.0369,  0.0026,  0.0227,  ..., -0.0089,  0.0262, -0.0003],\n",
       "         [ 0.0102,  0.0306,  0.0119,  ..., -0.0081, -0.0005,  0.0325],\n",
       "         ...,\n",
       "         [-0.0071,  0.0397,  0.0309,  ..., -0.0218,  0.0413, -0.0197],\n",
       "         [ 0.0132, -0.0027, -0.0330,  ...,  0.0066,  0.0041,  0.0168],\n",
       "         [-0.0158,  0.0156,  0.0321,  ..., -0.0266,  0.0086, -0.0383]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0237,  0.0300, -0.0144,  ..., -0.0078,  0.0047, -0.0067],\n",
       "         [ 0.0129,  0.0058, -0.0103,  ..., -0.0047, -0.0464, -0.0029],\n",
       "         [-0.0336, -0.0140,  0.0062,  ..., -0.0298,  0.0082, -0.0007],\n",
       "         ...,\n",
       "         [ 0.0125, -0.0205,  0.0183,  ...,  0.0165, -0.0006, -0.0266],\n",
       "         [ 0.0179, -0.0161, -0.0138,  ...,  0.0199, -0.0331,  0.0010],\n",
       "         [ 0.0069,  0.0014, -0.0269,  ...,  0.0004,  0.0015, -0.0009]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0060, -0.0430,  0.0155,  ...,  0.0272, -0.0117, -0.0056],\n",
       "         [ 0.0280, -0.0247,  0.0020,  ..., -0.0012, -0.0268,  0.0255],\n",
       "         [-0.0133,  0.0091,  0.0247,  ..., -0.0150, -0.0090,  0.0184],\n",
       "         ...,\n",
       "         [ 0.0077, -0.0057, -0.0075,  ...,  0.0391,  0.0252, -0.0043],\n",
       "         [ 0.0104, -0.0177, -0.0051,  ...,  0.0084,  0.0020,  0.0234],\n",
       "         [ 0.0110,  0.0316, -0.0090,  ..., -0.0044,  0.0024, -0.0117]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0236,  0.0045,  0.0286,  ..., -0.0169,  0.0071,  0.0024],\n",
       "         [-0.0263, -0.0066, -0.0055,  ...,  0.0266,  0.0426, -0.0575],\n",
       "         [ 0.0491,  0.0242,  0.0119,  ..., -0.0096,  0.0064, -0.0103],\n",
       "         ...,\n",
       "         [ 0.0204,  0.0222,  0.0385,  ..., -0.0092, -0.0011,  0.0273],\n",
       "         [-0.0076,  0.0043, -0.0049,  ..., -0.0094, -0.0091,  0.0234],\n",
       "         [ 0.0046, -0.0088,  0.0200,  ...,  0.0147, -0.0029, -0.0123]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0105,  0.0065,  0.0023,  ...,  0.0196,  0.0269,  0.0219],\n",
       "         [ 0.0292, -0.0159, -0.0002,  ...,  0.0176, -0.0294, -0.0246],\n",
       "         [-0.0735, -0.0071,  0.0245,  ...,  0.0111, -0.0157,  0.0023],\n",
       "         ...,\n",
       "         [ 0.0086, -0.0082,  0.0156,  ..., -0.0368,  0.0115,  0.0153],\n",
       "         [-0.0230,  0.0319,  0.0239,  ...,  0.0008, -0.0320, -0.0105],\n",
       "         [ 0.0238,  0.0058,  0.0139,  ...,  0.0210,  0.0031,  0.0102]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0039,  0.0041, -0.0029,  ..., -0.0007, -0.0172, -0.0273],\n",
       "         [ 0.0175, -0.0035,  0.0025,  ...,  0.0111,  0.0200,  0.0333],\n",
       "         [ 0.0189, -0.0013,  0.0105,  ..., -0.0185, -0.0101,  0.0305],\n",
       "         ...,\n",
       "         [ 0.0330, -0.0011,  0.0031,  ...,  0.0039, -0.0256,  0.0028],\n",
       "         [ 0.0169,  0.0254,  0.0109,  ..., -0.0247, -0.0043, -0.0442],\n",
       "         [ 0.0165, -0.0138, -0.0277,  ..., -0.0379, -0.0125, -0.0154]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0077,  0.0097,  0.0159,  ...,  0.0248, -0.0295, -0.0026],\n",
       "         [ 0.0253, -0.0130,  0.0038,  ..., -0.0025,  0.0066,  0.0454],\n",
       "         [ 0.0044, -0.0715, -0.0202,  ...,  0.0044,  0.0074,  0.0144],\n",
       "         ...,\n",
       "         [ 0.0375, -0.0037, -0.0032,  ..., -0.0244, -0.0135,  0.0111],\n",
       "         [-0.0526,  0.0360,  0.0138,  ...,  0.0152, -0.0007,  0.0113],\n",
       "         [-0.0215,  0.0263, -0.0187,  ..., -0.0212, -0.0084,  0.0523]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0184, -0.0159,  0.0179,  ..., -0.0526, -0.0101,  0.0097],\n",
       "         [ 0.0127, -0.0213,  0.0063,  ..., -0.0034,  0.0216, -0.0033],\n",
       "         [ 0.0026, -0.0233,  0.0037,  ...,  0.0104,  0.0098,  0.0027],\n",
       "         ...,\n",
       "         [ 0.0194,  0.0253, -0.0163,  ...,  0.0243,  0.0043,  0.0028],\n",
       "         [ 0.0351, -0.0332,  0.0083,  ...,  0.0143, -0.0015,  0.0192],\n",
       "         [ 0.0065,  0.0239, -0.0081,  ...,  0.0013,  0.0192, -0.0064]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.9169e-02, -2.2346e-02,  9.7766e-04,  ..., -1.7308e-02,\n",
       "          -1.3988e-02,  3.4728e-03],\n",
       "         [-1.5609e-02, -5.2530e-03, -2.9990e-02,  ..., -3.0613e-02,\n",
       "           1.4991e-02,  1.1643e-02],\n",
       "         [-9.7961e-03, -1.3066e-02, -1.5754e-02,  ..., -5.9148e-03,\n",
       "           2.4949e-02,  1.9248e-03],\n",
       "         ...,\n",
       "         [ 1.9093e-05, -1.2512e-03,  1.7154e-03,  ..., -2.2245e-02,\n",
       "          -3.9896e-03, -2.8999e-03],\n",
       "         [-2.4720e-04, -1.1075e-02,  9.2337e-03,  ...,  2.2873e-02,\n",
       "           4.1740e-02, -3.9407e-02],\n",
       "         [ 3.0179e-02, -1.6929e-03, -1.3344e-02,  ..., -3.5569e-02,\n",
       "           1.9167e-02,  1.1333e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0195,  0.0089,  0.0238,  ..., -0.0094, -0.0071, -0.0204],\n",
       "         [-0.0364,  0.0031, -0.0194,  ...,  0.0086,  0.0199, -0.0112],\n",
       "         [ 0.0163,  0.0038, -0.0297,  ...,  0.0293, -0.0500, -0.0096],\n",
       "         ...,\n",
       "         [ 0.0416,  0.0246,  0.0364,  ...,  0.0145,  0.0070, -0.0206],\n",
       "         [-0.0078,  0.0216,  0.0022,  ..., -0.0042,  0.0262, -0.0278],\n",
       "         [-0.0302,  0.0386,  0.0114,  ..., -0.0266, -0.0160,  0.0060]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0030,  0.0222, -0.0151,  ...,  0.0012,  0.0054,  0.0235],\n",
       "         [ 0.0251, -0.0014, -0.0267,  ...,  0.0202,  0.0206,  0.0344],\n",
       "         [ 0.0068,  0.0248,  0.0002,  ...,  0.0305,  0.0246,  0.0268],\n",
       "         ...,\n",
       "         [-0.0210, -0.0013,  0.0304,  ...,  0.0229,  0.0418, -0.0038],\n",
       "         [-0.0140, -0.0077, -0.0111,  ...,  0.0122, -0.0014, -0.0159],\n",
       "         [ 0.0155, -0.0220,  0.0316,  ..., -0.0520,  0.0261, -0.0343]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 4.1589e-03, -8.2105e-03, -6.4000e-02,  ...,  2.3573e-02,\n",
       "          -2.0342e-02, -1.4798e-02],\n",
       "         [ 2.9174e-02,  3.1281e-02,  1.3813e-02,  ..., -9.3326e-04,\n",
       "           1.5327e-02, -2.4873e-03],\n",
       "         [ 3.6240e-02, -2.1630e-02,  3.0756e-02,  ..., -3.1771e-02,\n",
       "           1.9511e-02, -1.2540e-02],\n",
       "         ...,\n",
       "         [ 2.9280e-02, -2.5335e-02, -2.7611e-03,  ...,  1.2688e-02,\n",
       "          -6.9367e-04,  1.0838e-02],\n",
       "         [ 7.7274e-05,  4.0684e-03,  4.1657e-02,  ..., -3.7070e-02,\n",
       "          -5.0722e-02,  2.7375e-02],\n",
       "         [-2.2431e-02,  2.1205e-02,  9.0831e-03,  ..., -3.6148e-03,\n",
       "          -1.7802e-02, -1.6481e-03]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0253,  0.0395, -0.0207,  ..., -0.0286,  0.0027, -0.0173],\n",
       "         [-0.0265,  0.0155, -0.0120,  ..., -0.0084,  0.0216, -0.0206],\n",
       "         [-0.0188, -0.0302,  0.0186,  ..., -0.0305,  0.0095, -0.0009],\n",
       "         ...,\n",
       "         [-0.0060, -0.0129,  0.0252,  ..., -0.0167,  0.0139,  0.0049],\n",
       "         [-0.0054, -0.0062, -0.0044,  ..., -0.0083,  0.0488, -0.0036],\n",
       "         [ 0.0009,  0.0213, -0.0224,  ...,  0.0069,  0.0213, -0.0083]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.6257e-02, -5.1508e-03,  2.8642e-02,  ..., -2.6078e-02,\n",
       "          -1.2408e-02, -5.8642e-03],\n",
       "         [-6.7722e-03,  2.0716e-02,  2.8329e-03,  ..., -5.2704e-02,\n",
       "          -2.5311e-02,  2.6384e-02],\n",
       "         [ 3.5670e-02,  1.4844e-03,  6.9353e-04,  ...,  6.8279e-03,\n",
       "          -7.6462e-03,  1.1964e-02],\n",
       "         ...,\n",
       "         [ 2.1803e-02,  3.2903e-05, -1.1143e-02,  ..., -6.0138e-03,\n",
       "          -2.5681e-02, -3.0746e-02],\n",
       "         [ 2.2904e-02, -1.8851e-02, -1.1962e-02,  ..., -2.0996e-02,\n",
       "           2.2485e-03,  3.2643e-02],\n",
       "         [ 1.1091e-02,  1.5224e-03, -7.1550e-03,  ..., -8.5266e-03,\n",
       "           2.9205e-03,  2.4818e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0173, -0.0032,  0.0028,  ...,  0.0203,  0.0018,  0.0086],\n",
       "         [ 0.0295,  0.0133, -0.0167,  ...,  0.0168, -0.0322, -0.0316],\n",
       "         [ 0.0080,  0.0063,  0.0208,  ..., -0.0128, -0.0056, -0.0183],\n",
       "         ...,\n",
       "         [ 0.0263, -0.0023,  0.0268,  ..., -0.0136, -0.0298,  0.0172],\n",
       "         [-0.0546, -0.0188, -0.0206,  ..., -0.0059,  0.0027, -0.0273],\n",
       "         [-0.0051, -0.0062, -0.0185,  ..., -0.0151,  0.0274, -0.0153]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0164,  0.0272, -0.0160,  ...,  0.0370,  0.0207,  0.0112],\n",
       "         [-0.0142,  0.0404,  0.0055,  ..., -0.0097,  0.0106,  0.0101],\n",
       "         [-0.0002, -0.0035, -0.0360,  ..., -0.0303,  0.0145,  0.0031],\n",
       "         ...,\n",
       "         [-0.0141, -0.0035, -0.0115,  ...,  0.0227, -0.0035, -0.0265],\n",
       "         [ 0.0125, -0.0161, -0.0408,  ..., -0.0102, -0.0150, -0.0454],\n",
       "         [-0.0216,  0.0105,  0.0177,  ..., -0.0269,  0.0118,  0.0020]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0373, -0.0115, -0.0033,  ..., -0.0141,  0.0013,  0.0007],\n",
       "         [-0.0075,  0.0174, -0.0296,  ...,  0.0020,  0.0388,  0.0181],\n",
       "         [ 0.0098,  0.0220,  0.0060,  ...,  0.0081,  0.0052, -0.0143],\n",
       "         ...,\n",
       "         [ 0.0041,  0.0079, -0.0025,  ...,  0.0097,  0.0095, -0.0180],\n",
       "         [ 0.0054, -0.0018, -0.0069,  ...,  0.0164,  0.0061,  0.0254],\n",
       "         [-0.0110,  0.0149,  0.0108,  ..., -0.0162, -0.0018,  0.0104]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0167, -0.0043,  0.0101,  ..., -0.0078, -0.0069, -0.0057],\n",
       "         [ 0.0195,  0.0096, -0.0082,  ...,  0.0149, -0.0110, -0.0035],\n",
       "         [-0.0251,  0.0044,  0.0041,  ...,  0.0248, -0.0150,  0.0078],\n",
       "         ...,\n",
       "         [-0.0113,  0.0350,  0.0080,  ..., -0.0226, -0.0024,  0.0082],\n",
       "         [-0.0385, -0.0006,  0.0023,  ...,  0.0287, -0.0293, -0.0149],\n",
       "         [ 0.0318, -0.0405, -0.0288,  ..., -0.0087, -0.0263,  0.0063]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0079,  0.0057, -0.0173,  ..., -0.0062,  0.0031, -0.0077],\n",
       "         [ 0.0005,  0.0286,  0.0250,  ..., -0.0026, -0.0208,  0.0278],\n",
       "         [-0.0068, -0.0073,  0.0530,  ...,  0.0147,  0.0103, -0.0220],\n",
       "         ...,\n",
       "         [ 0.0030, -0.0217, -0.0155,  ...,  0.0103,  0.0043,  0.0088],\n",
       "         [ 0.0082, -0.0196,  0.0104,  ...,  0.0082, -0.0394, -0.0139],\n",
       "         [-0.0009, -0.0083,  0.0084,  ..., -0.0240, -0.0274, -0.0042]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0028,  0.0019, -0.0361,  ...,  0.0218, -0.0032, -0.0037],\n",
       "         [-0.0088,  0.0045,  0.0146,  ...,  0.0037, -0.0314,  0.0008],\n",
       "         [-0.0220,  0.0036,  0.0074,  ..., -0.0077, -0.0211, -0.0008],\n",
       "         ...,\n",
       "         [ 0.0244,  0.0134, -0.0371,  ..., -0.0036,  0.0117, -0.0029],\n",
       "         [ 0.0091,  0.0181,  0.0171,  ..., -0.0296,  0.0003, -0.0057],\n",
       "         [ 0.0343,  0.0239, -0.0344,  ...,  0.0071,  0.0013, -0.0140]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda p: p.requires_grad, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 128256)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokenizer.vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8192 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 6])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2, 5, 6, 6)\n",
    "torch.diagonal(x, offset=0, dim1=-2, dim2=-1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0],\n",
       "        [3, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2, 1])\n",
    "b = torch.tensor([[1, 0], [3, 1]])\n",
    "\n",
    "torch.einsum('i,ij->ij', a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[2, 0], [0, 3]])\n",
    "b = torch.tensor([[1, 0], [0, 1]])\n",
    "\n",
    "torch.einsum('ij,ij->i', a, b)\n",
    "\n",
    "diag_a = torch.diagonal(a, dim1=-1, dim2=-2) \n",
    "diag_a = torch.tensor([-1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(*a.shape[-2:], dtype=torch.float32)[..., None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([a, b], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (arch_mod.py, line 151)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m/opt/miniconda3/envs/try-tad/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3667\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom aux.arch_mod import Mods\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m~/ml/skoltech/research_2/try_tad/aux/arch_mod.py:151\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mknn_logit = knn_logit.view(hidden _state.shape[:-1] + (-1,))\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from aux.arch_mod import Mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "try-tad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
