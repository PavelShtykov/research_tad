This thesis explores a potential interpretative framework for transformer language models based on movement through embedding space. Current approaches often separate token embeddings from hidden states, potentially limiting understanding of these models.

The proposed framework considers hidden states as points within the embedding space, possibly forming a path from token to token. This perspective may offer an alternative view where each layer contributes to a trajectory culminating in next-token prediction.

Experiments with Llama models and custom-trained models suggest that weight-tied models show alignment between similarity metrics and model logits. Analysis indicates intermediate representations seem to diverge from the token embedding manifold in deeper layers. A regularization approach appears to constrain hidden states without significantly impacting performance.

These preliminary findings may support the framework's potential, though questions remain. The ability to train models with representation constraints without apparent performance degradation might suggest directions for more interpretable language models.
