Transformer-based language models have revolutionized natural language processing, yet their internal mechanisms remain poorly understood. This thesis presents a novel interpretative framework that conceptualizes transformer operation as movement in embedding space, where each layer contributes to a trajectory guided by attention mechanisms and feed-forward networks. We investigate this framework through three complementary approaches. First, we analyze the relationship between probability distributions from the language modeling head and distances between hidden states and token embeddings in pre-trained LLAMA models (1B-7B), finding remarkably high correlations with NDCG scores exceeding 0.98. Second, we develop KNN-based alternatives to the traditional language modeling head that explicitly model distances in embedding space, reducing parameter count by 99.9\% while maintaining or improving performance. Third, we propose a first-layer-only attention mechanism that reduces inference memory requirements by a factor equal to the number of layers with minimal performance impact. Our experimental results provide strong evidence for the validity of our geometric interpretation, demonstrating that transformer predictions are closely related to proximity in embedding space. This interpretation not only offers a more intuitive understanding of how transformers process sequential information but also suggests architectural modifications that can significantly improve efficiency. By reframing transformer operation as movement in embedding space, this research contributes to both the theoretical understanding of these models and their practical implementation, offering insights that can guide the development of more interpretable and efficient architectures for natural language processing.
