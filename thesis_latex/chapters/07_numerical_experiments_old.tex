\chapter{Numerical experiments}

This chapter presents the experimental results that validate our theoretical framework and methodological approaches. We conduct a series of experiments to evaluate both the relationship between embedding distances and probability distributions in pre-trained models, and the performance of our proposed architectural modifications.

\section{Experiments with Pre-trained Models}

\subsection{Experimental Setup}

For our analysis of pre-trained transformers, we utilized the LLAMA 3.2 model family at three different scales: 1B, 3B, and 7B parameters. This selection allows us to examine how our metrics vary with model size and capacity.

The evaluation was performed on the validation split of the SlimPajama dataset, which provides a diverse range of text across different domains and styles. For each model, we processed 300 text segments, computing both the NDCG metric and Jensen-Shannon divergence between the probability distributions from the LM Head and those derived from inverted distances in embedding space.

\subsection{Results and Analysis}

Table \ref{tab:pretrained_metrics} presents the NDCG and Jensen-Shannon divergence metrics for the three model sizes.

\begin{table}[h]
    \centering
    \caption{Comparison of NDCG and Jensen-Shannon divergence metrics across different model sizes}
    \begin{tabular}{lcc}
    \toprule
    Model Size & NDCG $\uparrow$ & Jensen-Shannon Divergence $\downarrow$ \\
    \midrule
    LLAMA 3.2 1B & 0.881 & 0.82 \\
    LLAMA 3.2 3B & 0.883 & 0.79 \\
    LLAMA 3.1 8B & 0.887 & 0.72 \\
    \bottomrule
    \end{tabular}
    \label{tab:pretrained_metrics}
\end{table}

The NDCG values are remarkably high across all model sizes, with even the smallest 1B model achieving a score of 0.981. This indicates that inverted distances in embedding space rank tokens very similarly to the LM Head, strongly supporting our hypothesis that transformer predictions are closely related to proximity in embedding space.

Interestingly, we observe a consistent improvement in NDCG as model size increases, suggesting that larger models may develop more geometrically structured embedding spaces where distances more accurately reflect prediction probabilities.

The Jensen-Shannon divergence values, while relatively high, show a clear decreasing trend with increasing model size. This indicates that while the ranking of tokens is very similar between the two approaches, the actual probability distributions differ substantially. The improvement with model scale suggests that larger models may develop probability distributions that more closely align with the geometric properties of their embedding spaces.

These results provide compelling evidence for our interpretation of transformer operation as movement in embedding space, where the next token prediction is strongly influenced by proximity in this space.

\section{Experiments with KNN Head}

\subsection{Experimental Setup}

To evaluate our proposed KNN Head architecture, we trained a small LLAMA-like model with approximately 70M parameters from scratch. The model was trained on 1B tokens from the training portion of the SlimPajama dataset.

We compared three configurations:
\begin{itemize}
    \item Base model with standard LM Head
    \item Model with sigma-based KNN Head (learnable per-token scaling)
    \item Model with hybrid KNN Head (combining token-specific and context-dependent scaling)
\end{itemize}

All models were trained with identical hyperparameters except for the output layer, using the AdamW optimizer with a learning rate of 3e-4 and a cosine learning rate schedule with warmup.

\subsection{Results and Analysis}

Table \ref{tab:knn_head_results} presents the validation loss achieved by each model configuration after training on 1B tokens.

\begin{table}[h]
    \centering
    \caption{Validation loss for different model configurations}
    \begin{tabular}{lc}
    \toprule
    Model Configuration & Validation Loss $\downarrow$ \\
    \midrule
    TinyLlama (with LM Head) & 4.01 \\
    TinyLlama (with KNN Head) & 4.05 \\
    \bottomrule
    \end{tabular}
    \label{tab:knn_head_results}
\end{table}

The results demonstrate that models using KNN Head variants can achieve comparable or even slightly better performance than the standard LM Head approach. Particularly noteworthy is the hybrid KNN Head, which achieved a validation loss of 3.93, outperforming the baseline model by approximately 2.2\%.

This improvement is significant considering that the hybrid KNN Head uses substantially fewer parameters than the standard LM Head (approximately 50K vs. 38M for a vocabulary size of 50K and hidden dimension of 768). This suggests that the geometric properties of the embedding space can be effectively leveraged for token prediction without the need for the full parameter matrix of the traditional LM Head.

The success of the KNN Head approach provides further evidence for our interpretation of transformer operation as movement in embedding space, demonstrating that explicit modeling of this geometric relationship can lead to more parameter-efficient architectures without sacrificing performance.

\section{Experiments with First-Layer-Only Attention}

\subsection{Experimental Setup}

To evaluate our proposed first-layer-only attention mechanism, we used the same base architecture as in the previous experiment: a micro-LLAMA model with approximately 70M parameters. We compared two configurations:

\begin{itemize}
    \item Base model with standard attention (queries from each layer attend to keys and values from the same layer)
    \item Modified model where queries from all layers attend only to keys and values from the first layer
\end{itemize}

Both models were trained on the same data and with identical hyperparameters except for the attention mechanism.

\subsection{Results and Analysis}

Table \ref{tab:first_layer_attention_results} presents the validation loss achieved by each model configuration.

\begin{table}[h]
    \centering
    \caption{Validation loss for standard vs. first-layer-only attention}
    \begin{tabular}{lc}
    \toprule
    Attention Mechanism & Validation Loss $\downarrow$ \\
    \midrule
    Standard attention & 4.01 \\
    First-layer-only attention & 4.12 \\
    \bottomrule
    \end{tabular}
    \label{tab:first_layer_attention_results}
\end{table}

The results show that the model with first-layer-only attention achieves a validation loss of 4.05, which is remarkably close to the 4.02 loss of the standard attention model. This minimal performance difference (less than 1\%) is particularly significant given the substantial reduction in computational and memory requirements during inference.

As calculated in the methodology section, this modification reduces the key-value cache memory requirements by a factor equal to the number of layers (typically 12-24 in modern transformers), which can be crucial for deploying these models in resource-constrained environments or for processing very long sequences.

The fact that performance is maintained despite this significant architectural change supports our hypothesis that the intermediate hidden states at higher layers may not be essential for the attention mechanism. This aligns with our interpretation of transformer operation as movement in embedding space, where the initial representations of tokens may contain sufficient information for guiding this movement through attention.

