\chapter{Problem Statement}

\section{Transformer Language Model Architecture}

Decoder-only transformer language models form the core architecture behind modern large models such as GPT \cite{brown2020language} and LLaMA \cite{touvron2023llama}. Unlike the original encoder-decoder design \cite{vaswani2017attention}, these models utilize only the decoder with causal (unidirectional) attention, making them particularly suitable for autoregressive language modeling tasks.

\begin{figure}[h]
    \centering
    % Placeholder for transformer schema
    \caption{Architecture of a decoder-only transformer model illustrating the processing pipeline: token embeddings are transformed through multiple decoder layers (each containing self-attention and feed-forward networks with residual connections), culminating in the language modeling head that produces next-token predictions.}
    \label{fig:transformer_architecture}
\end{figure}

A decoder-only transformer begins with a vocabulary $\mathcal{V}$, where each token is mapped to a $d$-dimensional embedding via matrix $\mathbf{E} \in \mathbb{R}^{|\mathcal{V}| \times d}$. For a sequence $\mathbf{t} = (t_1, \ldots, t_n)$, the embedding layer produces a sequence of vectors, which are then processed by $L$ stacked decoder layers, yielding hidden states $\mathbf{h}_i^l \in \mathbb{R}^d$ for each token $i$ at each layer $l$.

Each decoder layer consists of two main components: multi-head self-attention and a position-wise feed-forward network (FFN), both implemented with residual connections and layer normalization:
\begin{equation}
\mathbf{h}' = \mathbf{h} + \text{Attn}(\text{Norm}_1(\mathbf{h}), \mathbf{H})
\end{equation}
\begin{equation}
\mathbf{h}^{\text{out}} = \mathbf{h}' + \text{FFN}(\text{Norm}_2(\mathbf{h}'))
\end{equation}

The self-attention mechanism enables each token to attend to itself and previous tokens (causal attention). For token $i$ at layer $l$:
\begin{equation}
    \text{Attn}(\mathbf{h}_i^{l-1}, \mathbf{H}^{l-1}) = \sum_{j=1}^{i} \alpha_{ij} \mathbf{W}_V \mathbf{h}_j^{l-1}
\end{equation}
where $\mathbf{H}^{l-1}$ represents the set of all hidden states at layer $l-1$, i.e., $\mathbf{H}^{l-1} = \{\mathbf{h}_1^{l-1}, \mathbf{h}_2^{l-1}, \ldots, \mathbf{h}_i^{l-1}\}$ for causal attention, and $\alpha_{ij}$ are attention weights computed as:
\begin{equation}
    \alpha_{ij} = \frac{\exp((\mathbf{W}_Q \mathbf{h}_i^{l-1})^T (\mathbf{W}_K \mathbf{h}_j^{l-1}) / \sqrt{d_k})}{\sum_{j'=1}^{i} \exp((\mathbf{W}_Q \mathbf{h}_i^{l-1})^T (\mathbf{W}_K \mathbf{h}_{j'}^{l-1}) / \sqrt{d_k})}
\end{equation}
Here, $\mathbf{W}_Q$, $\mathbf{W}_K$, $\mathbf{W}_V$ are learned parameter matrices, and $d_k$ is the key dimension.

The FFN is applied to each position independently:
\begin{equation}
    \text{FFN}(\mathbf{h}_i^{l-1}) = \mathbf{W}_2 \max(0, \mathbf{W}_1 \mathbf{h}_i^{l-1} + \mathbf{b}_1) + \mathbf{b}_2
\end{equation}
where $\mathbf{W}_1$, $\mathbf{W}_2$, $\mathbf{b}_1$, $\mathbf{b}_2$ are learned parameters.

The final hidden state $\mathbf{h}_i^L$ is projected to logits over the vocabulary by a linear language modeling head:
\begin{equation}
    \mathbf{z}_i = \text{LmHead}(\mathbf{h}_i^L) = \mathbf{W} \mathbf{h}_i^L + \mathbf{b}
    \label{eq::lm_head}
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{|\mathcal{V}| \times d}$ and $\mathbf{b} \in \mathbb{R}^{|\mathcal{V}|}$. The probability of the next token is given by softmax:
\begin{equation}
    P(t_{i+1} = v | t_1, \ldots, t_i) = \frac{\exp(z_{i,v})}{\sum_{v' \in \mathcal{V}} \exp(z_{i,v'})}
    \label{eq::softmax}
\end{equation}

This architecture enables effective modeling of sequential data with both local and long-range dependencies, making decoder-only transformers highly effective for natural language processing tasks.


\section{Embedding Space Interpretation Challenge}

Current approaches to transformer interpretation often create an artificial separation between token embeddings $\mathbf{E}$ and intermediate hidden states $\mathbf{h}_i^l$. This separation introduces a conceptual disconnect in understanding how transformers process information. A more unified perspective is proposed: treating hidden states as points within the same embedding space. In this framework, the sequence of hidden states forms a continuous path from one token to the next.

The attention mechanism and feed-forward networks (described in the previous section) construct this path incrementally. Each layer transforms the representation, moving it closer to the next token prediction. The final output, calculated through Equations \ref{eq::lm_head} and \ref{eq::softmax}, represents the culmination of this trajectory through embedding space.

This unified perspective offers a more intuitive understanding of transformer operations and may lead to valuable insights for model design and optimization. The subsequent chapters explore this interpretation in detail, providing both theoretical analysis and empirical validation.
