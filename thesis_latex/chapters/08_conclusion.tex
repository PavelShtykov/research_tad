\chapter{Discussion and Conclusion}

This thesis explored a potential interpretative framework for transformer language models based on the concept of movement through embedding space. The research attempted to address a gap in current approaches to transformer interpretation, which typically separate token embeddings from hidden states.

The proposed framework considers hidden states as points within the embedding space, potentially forming a path from token to token. This perspective may offer an alternative understanding of transformer operations, where each layer could contribute to a trajectory through embedding space.

Experimental results suggest several possible insights:
\begin{itemize}
    \item Models with weight tying appeared to show alignment between similarity-based rankings and model logits, though the relationship's nature requires further investigation.
    \item Different language modeling head architectures exhibited relatively similar performance, potentially indicating some degree of robustness.
    \item Hidden state norms in pre-trained models seemed to increase with layer depth, possibly indicating divergence from the token embedding space.
    \item A regularization approach appeared to constrain intermediate hidden states without major performance impacts, though its broader implications remain unclear.
\end{itemize}

These preliminary findings may support the potential of the proposed framework, though many questions remain unanswered. The ability to train models with representation constraints without apparent performance degradation might suggest directions for more interpretable models.

Future work could explore this approach with larger models, investigate regularization strategies, and examine the framework's applicability to various transformer architectures.
