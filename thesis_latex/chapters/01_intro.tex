\chapter{Introduction}

\paragraph{Relevance.}
Transformer-based language models have revolutionized natural language processing, yet our understanding of their internal representations remains limited. Current approaches often create an artificial separation between token embeddings and hidden states, hindering interpretability. As these models become increasingly integrated into critical systems, developing better interpretative frameworks is essential for ensuring reliability, enabling systematic debugging, and driving architectural innovations.

\paragraph{Main purpose of the research.}
This research aims to develop and validate a novel interpretative framework for transformer language models based on the concept of movement through embedding space. The central hypothesis posits that transformer operations can be understood as a trajectory from one token to the next, with each layer contributing to this path. Through analysis of pre-trained models and experiments with models trained under specific constraints, this work investigates whether this embedding space movement paradigm offers a valid perspective for understanding transformer behavior.

\paragraph{Scientific novelty.}
This thesis explores potential contributions to transformer interpretation: (1) a perspective that considers both token embeddings and hidden states as points within the same embedding space; (2) methods to examine this interpretative framework; (3) an experimental regularization approach that attempts to constrain intermediate hidden states within the token embedding manifold; and (4) preliminary evidence from both pre-trained models and models with alternative architectural configurations. While these contributions suggest possible directions for transformer interpretation, they represent initial steps that require further validation and refinement.

\paragraph{Statements for defense.}
The following statements are presented for defense:

1. The proposed interpretative framework of movement through embedding space offers a valid perspective for understanding transformer language models.

2. In standard pre-trained transformers, intermediate hidden states progressively diverge from the token embedding manifold as layer depth increases.

3. Intermediate hidden states can be constrained to remain within the vicinity of the token embedding cloud through regularization without significant performance degradation.

4. Different language modeling head architectures, including those explicitly based on similarity metrics, can achieve comparable performance.
