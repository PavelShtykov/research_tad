\chapter{Author contribution}

In this thesis, I have made the following contributions to the research presented:

\begin{itemize}
    \item \textbf{Conceptualization:} I formulated the novel interpretation of transformer operation as movement in embedding space and developed the theoretical framework that formalizes this interpretation as a trajectory guided by attention mechanisms and feed-forward networks.
    
    \item \textbf{Methodology:} I designed approaches to validate this interpretation, including analysis frameworks for embedding distances in pre-trained models, KNN-based alternatives to traditional language modeling heads, and a first-layer-only attention mechanism for efficient inference.
    
    \item \textbf{Implementation:} I implemented all necessary code, including modifications to transformer architectures to support KNN-based heads, custom attention mechanisms, and metrics for comparing probability distributions and embedding space properties.
    
    \item \textbf{Experimentation:} I conducted experiments analyzing pre-trained LLAMA models (1B-7B) and trained models from scratch with KNN-based heads and modified attention mechanisms to validate my hypotheses.
    
    \item \textbf{Analysis:} I analyzed the experimental results, quantifying correlations between embedding distances and probabilities, evaluating parameter efficiency, and assessing memory savings of the proposed architectural modifications.
\end{itemize}

This research was conducted under the supervision of Serguei Barannikov. I acknowledge the use of open-source libraries, pre-trained LLAMA models, and the SlimPajama dataset that supported this work.
