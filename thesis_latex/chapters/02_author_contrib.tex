\chapter{Author contribution}

In this thesis, I have made the following contributions to the research presented:

\begin{itemize}
    \item \textbf{Conceptualization:} I formulated the research questions and hypotheses regarding the relationship between transformer hidden states and token embeddings. I proposed the interpretation of transformer operation as inertial movement in embedding space, providing a novel framework for understanding these models.
    
    \item \textbf{Methodology:} I designed the experimental methodology to test the relationship between probability distributions from the language modeling head and distances between hidden states and token embeddings. Additionally, I developed the approach for replacing the traditional linear language modeling head with a learnable Gaussian kernel over k-nearest neighbors.
    
    \item \textbf{Implementation:} I implemented all code necessary for the experiments, including modifications to transformer architectures to support alternative language modeling heads and to extract internal representations for analysis. This implementation work included developing custom PyTorch modules for the Gaussian kernel approach and metrics for comparing probability distributions.
    
    \item \textbf{Experimentation:} I conducted all experiments described in this thesis, including training modified transformer models and analyzing the geometric properties of transformer hidden states in relation to token embeddings.
    
    \item \textbf{Analysis:} I performed the analysis of experimental results, including the calculation of NDCG scores between probability distributions and the interpretation of these results in the context of the proposed theoretical framework.
\end{itemize}

All aspects of this research were conducted under the supervision of Serguei Barannikov, who provided guidance on the theoretical foundations and experimental design. I acknowledge the use of open-source libraries and pre-trained models that served as the foundation for the experimental work.
