@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{aken2020visbert,
      title={VisBERT: Hidden-State Visualizations for Transformers}, 
      author={Betty van Aken and Benjamin Winter and Alexander LÃ¶ser and Felix A. Gers},
      year={2020},
      eprint={2011.04507},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2011.04507}
}

@article{dar2022analyzing,
      title={Analyzing Transformers in Embedding Space}, 
      author={Guy Dar and Mor Geva and Ankit Gupta and Jonathan Berant},
      year={2023},
      eprint={2209.02535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.02535}, 
}

@article{singh2023analyzing,
      title={Analyzing Transformer Dynamics as Movement through Embedding Space}, 
      author={Sumeet S. Singh},
      year={2023},
      eprint={2308.10874},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.10874}, 
}

@inproceedings{valeriani2023geometry,
      title={The geometry of hidden representations of large transformer models}, 
      author={Lucrezia Valeriani and Diego Doimo and Francesca Cuturello and Alessandro Laio and Alessio Ansuini and Alberto Cazzaniga},
      year={2023},
      eprint={2302.00294},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.00294}, 
}

@article{geva2022transformer,
      title={Transformer Feed-Forward Layers Are Key-Value Memories}, 
      author={Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
      year={2021},
      eprint={2012.14913},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.14913}, 
}

@article{haris2024knn,
      title={$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers}, 
      author={Themistoklis Haris},
      year={2024},
      eprint={2411.04013},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.04013}, 
}

@article{chowdhury2021learning,
      title={On Learning the Transformer Kernel}, 
      author={Sankalan Pal Chowdhury and Adamos Solomou and Avinava Dubey and Mrinmaya Sachan},
      year={2022},
      eprint={2110.08323},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.08323}, 
}

@inproceedings{simpson2021kernel,
      title={Kernel Identification Through Transformers}, 
      author={Fergus Simpson and Ian Davies and Vidhi Lalchand and Alessandro Vullo and Nicolas Durrande and Carl Rasmussen},
      year={2021},
      eprint={2106.08185},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2106.08185}, 
}

@article{song2023uncovering,
      title={Uncovering hidden geometry in Transformers via disentangling position and context}, 
      author={Jiajun Song and Yiqiao Zhong},
      year={2024},
      eprint={2310.04861},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.04861}, 
}


% new ----------------------


@article{for_scheme_base,
      title={Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning}, 
      author={Georgia Chalvatzaki and Ali Younes and Daljeet Nandha and An Le and Leonardo F. R. Ribeiro and Iryna Gurevych},
      year={2023},
      eprint={2305.07716},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2305.07716}, 
}



@article{math_framework_transformer,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}


@inproceedings{torch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@software{torch-lightning,
author = {Falcon, William and {The PyTorch Lightning team}},
doi = {10.5281/zenodo.3828935},
license = {Apache-2.0},
month = mar,
title = {{PyTorch Lightning}},
url = {https://github.com/Lightning-AI/lightning},
version = {1.4},
year = {2019}
}

@article{llama32,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
 

@misc{slimpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = June,
  year = 2023,
  howpublished = {\url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
  url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}


@misc{mmlu,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{agieval,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.06364}, 
}

@misc{SQuAD,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1606.05250}, 
}

@misc{scaling_law,
      title={Reconciling Kaplan and Chinchilla Scaling Laws}, 
      author={Tim Pearce and Jinyeop Song},
      year={2024},
      eprint={2406.12907},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.12907}, 
}